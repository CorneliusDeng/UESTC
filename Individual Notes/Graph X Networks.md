# Introduction

**[A Gentle Introduction to Graph Neural Networks](https://distill.pub/2021/gnn-intro/)**

**[Understanding Convolutions on Graphs](https://distill.pub/2021/understanding-gnns/)**

**[Build your first Graph Neural Network model to predict traffic speed in 20 minutes](https://towardsdatascience.com/build-your-first-graph-neural-network-model-to-predict-traffic-speed-in-20-minutes-b593f8f838e5)**

**[CNN-explainer](https://poloclub.github.io/cnn-explainer/)**

[**Fast Fourier Transform**](https://zhuanlan.zhihu.com/p/31584464)

[**å¦‚ä½•ç†è§£ Graph Convolutional Networks**](https://www.zhihu.com/question/54504471/answer/332657604)

[**å¦‚ä½•ç†è§£ Graph Attention Networks**](https://zhuanlan.zhihu.com/p/81350196)

Graphs are all around us; real world objects are often defined in terms of their connections to other things. A set of objects, and the connections between them, are naturally expressed as a graph.

Researchers have developed neural networks that operate on graph data (called **graph neural networks, or GNNs**) for over a decade.




# Graph

A graph represents the relations (*edges*) between a collection of entities (*nodes*).

- Three types of attributes we might find in a graph
  - Vertex (or node) attributes
  - Edge (or link) attributes and directions
  - Global (or master node) attributes
- To further describe each node, edge or the entire graph, we can store information in each of these pieces of the graph.
  - Vertex (or node) embedding
  - Edge (or link) attributes and embedding
  - Global (or master node) embedding
- We can additionally specialize graphs by associating directionality to edges (*directed, undirected*).
  - The edges can be directed, where an edge $ğ‘’$ has a source node, $ğ‘£_{ğ‘ ğ‘Ÿğ‘}$, and a destination node $ğ‘£_{ğ‘‘ğ‘ ğ‘¡}$. In this case, information flows from $ğ‘£_{ğ‘ ğ‘Ÿğ‘}$ to $ğ‘£_{ğ‘‘ğ‘ ğ‘¡}$.
  - They can also be undirected, where there is no notion of source or destination nodes, and information flows both directions.

## Images & Text as graphs

We typically think of images as rectangular grids with image channels, representing them as arrays (e.g., 244x244x3 floats). Another way to think of images is as graphs with regular structure, where each pixel represents a node and is connected via an edge to adjacent pixels. Each non-border pixel has exactly 8 neighbors, and the information stored at each node is a 3-dimensional vector representing the RGB value of the pixel.

A way of visualizing the connectivity of a graph is through its *adjacency matrix*.

------

We can digitize text by associating indices to each character, word, or token, and representing text as a sequence of these indices. This creates a simple directed graph, where each character or index is a node and is connected via an edge to the node that follows it.

This representation (a sequence of character tokens) refers to the way text is often represented in RNNs; other models, such as Transformers, can be considered to view text as a fully connected graph where we learn the relationship between tokens. 

------

Of course, in practice, this is not usually how text and images are encoded: these graph representations are redundant since all images and all text will have very regular structures. For instance, images have a banded structure in their adjacency matrix because all nodes (pixels) are connected in a grid. The adjacency matrix for text is just a diagonal line, because each word only connects to the prior word, and to the next one.

## Graph-valued data in the wild

**Molecules as graphs.** Molecules are the building blocks of matter, and are built of atoms and electrons in 3D space. All particles are interacting, but when a pair of atoms are stuck in a stable distance from each other, we say they share a covalent bond. Different pairs of atoms and bonds have different distances (e.g. single-bonds, double-bonds). Itâ€™s a very convenient and common abstraction to describe this 3D object as a graph, where nodes are atoms and edges are covalent bonds.

**Social networks as graphs.** Social networks are tools to study patterns in collective behaviour of people, institutions and organizations. We can build a graph representing groups of people by modelling individuals as nodes, and their relationships as edges.

**Citation networks as graphs.** Scientists routinely cite other scientistsâ€™ work when publishing papers. We can visualize these networks of citations as a graph, where each paper is a node, and each *directed* edge is a citation between one paper and another. Additionally, we can add information about each paper into each node, such as a word embedding of the abstract. 

**Other examples.**In computer vision, we sometimes want to tag objects in visual scenes. We can then build graphs by treating these objects as nodes, and their relationships as edges. Machine learning models, programming code and math equations can also be phrased as graphs, where the variables are nodes, and edges are operations that have these variables as input and output. You might see the term â€œdataflow graphâ€ used in some of these contexts.

The structure of real-world graphs can vary greatly between different types of dataâ€‰â€”â€‰some graphs have many nodes with few connections between them, or vice versa. Graph datasets can vary widely (both within a given dataset, and between datasets) in terms of the number of nodes, edges, and the connectivity of nodes.

## Types of prediction tasks on graphs

There are three general types of prediction tasks on graphs: graph-level, node-level, and edge-level.

In a graph-level task, we predict a single property for a whole graph. For a node-level task, we predict some property for each node in a graph. For an edge-level task, we want to predict the property or presence of edges in a graph.

### Graph-level task

In a graph-level task, our goal is to predict the property of an entire graph. For example, for a molecule represented as a graph, we might want to predict what the molecule smells like, or whether it will bind to a receptor implicated in a disease.

This is analogous to image classification problems with MNIST and CIFAR, where we want to associate a label to an entire image. With text, a similar problem is sentiment analysis where we want to identify the mood or emotion of an entire sentence at once.

### Node-level task

Node-level tasks are concerned with predicting the identity or role of each node within a graph.

A classic example of a node-level prediction problem is Zachâ€™s karate club. The dataset is a single social network graph made up of individuals that have sworn allegiance to one of two karate clubs after a political rift. As the story goes, a feud between Mr. Hi (Instructor) and John H (Administrator) creates a schism in the karate club. The nodes represent individual karate practitioners, and the edges represent interactions between these members outside of karate. The prediction problem is to classify whether a given member becomes loyal to either Mr. Hi or John H, after the feud. In this case, distance between a node to either the Instructor or Administrator is highly correlated to this label.

Following the image analogy, node-level prediction problems are analogous to *image segmentation*, where we are trying to label the role of each pixel in an image. With text, a similar task would be predicting the parts-of-speech of each word in a sentence (e.g. noun, verb, adverb, etc).

### Edge-level task

One example of edge-level inference is in image scene understanding. Beyond identifying objects in an image, deep learning models can be used to predict the relationship between them. We can phrase this as an edge-level classification: given nodes that represent the objects in the image, we wish to predict which of these nodes share an edge or what the value of that edge is. If we wish to discover connections between entities, we could consider the graph fully connected and based on their predicted value prune edges to arrive at a sparse graph.

<img src="https://distill.pub/2021/gnn-intro/merged.0084f617.png" style="zoom: 33%;" />

<img src="https://distill.pub/2021/gnn-intro/edges_level_diagram.c40677db.png" style="zoom:50%;" />



## The challenges of using graphs in machine learning

So, how do we go about solving these different graph tasks with neural networks? 

The first step is to think about how we will represent graphs to be compatible with neural networks.

Machine learning models typically take rectangular or grid-like arrays as input. So, itâ€™s not immediately intuitive how to represent them in a format that is compatible with deep learning. Graphs have up to four types of information that we will potentially want to use to **make predictions: nodes, edges, global-context and connectivity.** The first three are relatively straightforward: for example, with nodes we can form a node feature matrix $N$ by assigning each node an index $i$ and storing the feature for $node_i$ in $N$. While these matrices have a variable number of examples, they can be processed without any special techniques.

However, representing a graphâ€™s connectivity is more complicated. Perhaps the most obvious choice would be to use an adjacency matrix, since this is easily tensorisable. However, this representation has a few drawbacks. From the example dataset table, we see the number of nodes in a graph can be on the order of millions, and the number of edges per node can be highly variable. Often, this leads to very sparse adjacency matrices, which are space-inefficient. 

Another problem is that there are many adjacency matrices that can encode the same connectivity, and there is no guarantee that these different matrices would produce the same result in a deep neural network (that is to say, they are not permutation invariant).

One elegant and memory-efficient way of representing sparse matrices is as **adjacency lists**. These describe the connectivity of edge $e_k$ between nodes $n_i$ and $n_j$ as a tuple $(i,j)$ in the k-th entry of an adjacency list. Since we expect the number of edges to be much lower than the number of entries for an adjacency matrix $n_{nodes}^2$ , we avoid computation and storage on the disconnected parts of the graph.

Most practical tensor representations have vectors per graph attribute(per node/edge/global). Instead of a node tensor of size $[n_{nodes}]$ we will be dealing with node tensors of size $[n_{nodes},node_{dim}]$. Same for the other graph attributes.



## The Challenges of Computation on Graphs

### Lack of Consistent Structure

Graphs are extremely flexible mathematical models; but this means they lack consistent structure across instances. 

Consider the task of predicting whether a given chemical molecule is toxic. Looking at a few examples, the following issues quickly become apparent:

- Molecules may have different numbers of atoms.
- The atoms in a molecule may be of different types.
- Each of these atoms may have different number of connections.
- These connections can have different strengths.

Representing graphs in a format that can be computed over is non-trivial, and the final representation chosen often depends significantly on the actual problem.

### Node-Order Equivariance

Extending the point above: graphs often have no inherent ordering present amongst the nodes. Compare this to images, where every pixel is uniquely determined by its absolute position within the image!

The same graph labelled in two different ways. The alphabets indicate the ordering of the nodes.

![](https://distill.pub/2021/understanding-gnns/images/node-order-alternatives.svg)

As a result, we would like our algorithms to be node-order equivariant: they should not depend on the ordering of the nodes of the graph. If we permute the nodes in some way, the resulting representations of the nodes as computed by our algorithms should also be permuted in the same way.

### Scalability

Graphs can be really large! Think about social networks like Facebook and Twitter, which have over a billion users. Operating on data this large is not easy.

Luckily, most naturally occuring graphs are â€˜sparseâ€™: they tend to have their number of edges linear in their number of vertices. We will see that this allows the use of clever methods to efficiently compute representations of nodes within the graph. Further, the methods that we look at here will have significantly fewer parameters in comparison to the size of the graphs they operate on.



# Graph Neural Networks

Now that the graphâ€™s description is in a matrix format that is permutation invariant, we will describe using graph neural networks (GNNs) to solve graph prediction tasks.

**A GNN is an optimizable transformation on all attributes of the graph (nodes, edges, global-context) that preserves graph symmetries (permutation invariances).** 

GNNs adopt a â€œgraph-in, graph-outâ€ architecture meaning that these model types accept a graph as input, with information loaded into its nodes, edges and global-context, and progressively transform these embeddings, without changing the connectivity of the input graph.

## The simplest GNN

We will start with the simplest GNN architecture, one where we learn new embeddings for all graph attributes (nodes, edges, global), but where we do not yet use the connectivity of the graph.

This GNN uses a separate multilayer perceptron (MLP) on each component of a graph; we call this a GNN layer. For each node vector, we apply the MLP and get back a learned node-vector. We do the same for each edge, learning a per-edge embedding, and also for the global-context vector, learning a single embedding for the entire graph.

![](https://distill.pub/2021/gnn-intro/arch_independent.0efb8ae7.png)

A single layer of a simple GNN. A graph is the input, and each component (V,E,U) gets updated by a MLP to produce a new graph. Each function subscript indicates a separate function for a different graph attribute at the n-th layer of a GNN model. 

As is common with neural networks modules or layers, we can stack these GNN layers together. Because a GNN does not update the connectivity of the input graph, we can describe the output graph of a GNN with the same adjacency list and the same number of feature vectors as the input graph. But, the output graph has updated embeddings, since the GNN has updated each of the node, edge and global-context representations.

## GNN Predictions by Pooling Information

We will consider the case of binary classification, but this framework can easily be extended to the multi-class or regression case. If the task is to make binary predictions on nodes, and the graph already contains node information, the approach is straightforwardâ€‰â€”â€‰for each node embedding, apply a linear classifier.

![](https://distill.pub/2021/gnn-intro/prediction_nodes_nodes.c2c8b4d0.png)

However, it is not always so simple. For instance, you might have information in the graph stored in edges, but no information in nodes, but still need to make predictions on nodes. We need a way to collect information from edges and give them to nodes for prediction. We can do this by *pooling*. 

- Pooling proceeds in two steps:
  - For each item to be pooled, *gather* each of their embeddings and concatenate them into a matrix.
  - The gathered embeddings are then *aggregated*, usually via a sum operation.

We represent the *pooling* operation by the letter $\rho$, and denote that we are gathering information from edges to nodes as  $\rho_{E_n\to V_n}$.

So If we only have edge-level features, and are trying to predict binary node information, we can use pooling to route (or pass) information to where it needs to go. The model looks like this.

<img src="https://distill.pub/2021/gnn-intro/prediction_edges_nodes.e6796b8e.png" style="zoom:10%;" />

If we only have node-level features, and are trying to predict binary edge-level information, the model looks like this.

<img src="https://distill.pub/2021/gnn-intro/prediction_nodes_edges.26fadbcc.png" style="zoom:10%;" />

If we only have node-level features, and need to predict a binary global property, we need to gather all available node information together and aggregate them. This is similar to *Global Average Pooling* layers in CNNs. The same can be done for edges.

<img src="https://distill.pub/2021/gnn-intro/prediction_nodes_edges_global.7a535eb8.png" style="zoom: 10%;" />

In our examples, the classification model $c$ can easily be replaced with any differentiable model, or adapted to multi-class classification using a generalized linear model.

This is an end-to-end prediction task with a GNN model.

![](https://distill.pub/2021/gnn-intro/Overall.e3af58ab.png)

Now weâ€™ve demonstrated that we can build a simple GNN model, and make binary predictions by routing information between different parts of the graph. This pooling technique will serve as a building block for constructing more sophisticated GNN models. If we have new graph attributes, we just have to define how to pass information from one attribute to another.

Note that in this simplest GNN formulation, weâ€™re not using the connectivity of the graph at all inside the GNN layer. Each node is processed independently, as is each edge, as well as the global context. We only use connectivity when pooling information for prediction.

## Passing messages between parts of the graph

We could make more sophisticated predictions by using pooling within the GNN layer, in order to make our learned embeddings aware of graph connectivity. We can do this using *message passing*, where neighboring nodes or edges exchange information and influence each otherâ€™s updated embeddings.

- Message passing works in three steps:
  - For each node in the graph, *gather* all the neighboring node embeddings (or messages), which is the $g$ function described above.
  - Aggregate all messages via an aggregate function (like sum).
  - All pooled messages are passed through an *update function*, usually a learned neural network.

Just as pooling can be applied to either nodes or edges, message passing can occur between either nodes or edges.

These steps are key for leveraging the connectivity of graphs. We will build more elaborate variants of message passing in GNN layers that yield GNN models of increasing expressiveness and power.

This sequence of operations, when applied once, is the simplest type of message-passing GNN layer.

This is reminiscent of standard convolution: in essence, message passing and convolution are operations to aggregate and process the information of an elementâ€™s neighbors in order to update the elementâ€™s value. In graphs, the element is a node, and in images, the element is a pixel. However, the number of neighboring nodes in a graph can be variable, unlike in an image where each pixel has a set number of neighboring elements.

By stacking message passing GNN layers together, a node can eventually incorporate information from across the entire graph: after three layers, a node has information about the nodes three steps away from it.

We can update our architecture diagram to include this new source of information for nodes. Schematic for a GCN architecture, which updates node representations of a graph by pooling neighboring nodes at a distance of one degree.

![](https://distill.pub/2021/gnn-intro/arch_gcn.40871750.png)

## Learning edge representations

Our dataset does not always contain all types of information (node, edge, and global context). When we want to make a prediction on nodes, but our dataset only has edge information, we showed above how to use pooling to route information from edges to nodes, but only at the final prediction step of the model. We can share information between nodes and edges within the GNN layer using message passing.

We can incorporate the information from neighboring edges in the same way we used neighboring node information earlier, by first pooling the edge information, transforming it with an update function, and storing it.

However, the node and edge information stored in a graph are not necessarily the same size or shape, so it is not immediately clear how to combine them. One way is to learn a linear mapping from the space of edges to the space of nodes, and vice versa. Alternatively, one may concatenate them together before the update function.

Architecture schematic for Message Passing layer. The first step â€œpreparesâ€ a message composed of information from an edge and itâ€™s connected nodes and then â€œpassesâ€ the message to the node.

![](https://distill.pub/2021/gnn-intro/arch_mpnn.a13c2294.png)

Which graph attributes we update and in which order we update them is one design decision when constructing GNNs. We could choose whether to update node embeddings before edge embeddings, or the other way around. This is an open area of research with a variety of solutionsâ€“ for example we could update in a â€˜weaveâ€™ fashion where we have four updated representations that get combined into new node and edge representations: node to node (linear), edge to edge (linear), node to edge (edge layer), edge to node (node layer).

Some of the different ways we might combine edge and node representation in a GNN layer:

<img src="https://distill.pub/2021/gnn-intro/arch_weave.352befc0.png"  />

## Adding global representations

There is one flaw with the networks we have described so far: nodes that are far away from each other in the graph may never be able to efficiently transfer information to one another, even if we apply message passing several times. For one node, If we have k-layers, information will propagate at most k-steps away. This can be a problem for situations where the prediction task depends on nodes, or groups of nodes, that are far apart. One solution would be to have all nodes be able to pass information to each other. Unfortunately for large graphs, this quickly becomes computationally expensive (although this approach, called â€˜virtual edgesâ€™, has been used for small graphs such as molecules).

One solution to this problem is by using the global representation of a graph (U) which is sometimes called a **master node** or context vector. This global context vector is connected to all other nodes and edges in the network, and can act as a bridge between them to pass information, building up a representation for the graph as a whole. This creates a richer and more complex representation of the graph than could have otherwise been learned.

Schematic of a Graph Nets architecture leveraging global representations:

![](https://distill.pub/2021/gnn-intro/arch_graphnet.b229be6d.png)

In this view all graph attributes have learned representations, so we can leverage them during pooling by conditioning the information of our attribute of interest with respect to the rest. For example, for one node we can consider information from neighboring nodes, connected edges and the global information. To condition the new node embedding on all these possible sources of information, we can simply concatenate them. Additionally we may also map them to the same space via a linear map and add them or apply a feature-wise modulation layer, which can be considered a type of featurize-wise attention mechanism.

Schematic for conditioning the information of one node based on three other embeddings (adjacent nodes, adjacent edges, global). This step corresponds to the node operations in the Graph Nets Layer.

![](https://distill.pub/2021/gnn-intro/graph_conditioning.3017e214.png)



## Problem Setting and Notation

There are many useful problems that can be formulated over graphs:

- **Node Classification:** Classifying individual nodes.
- **Graph Classification:** Classifying entire graphs.
- **Node Clustering:** Grouping together similar nodes based on connectivity.
- **Link Prediction:** Predicting missing links.
- **Influence Maximization:** Identifying influential nodes.
- $\cdots \cdots$

<img src="https://distill.pub/2021/understanding-gnns/images/graph-tasks.svg"  />

A common precursor in solving many of these problems is **node representation learning**: learning to map individual nodes to fixed-size real-valued vectors (called â€˜representationsâ€™ or â€˜embeddingsâ€™).

Different GNN variants are distinguished by the way these representations are computed. Generally, however, GNNs compute node representations in an iterative process. We will use the notation $h_v^{(k)}$ to indicate the representation of node $v$ after the $k^{th}$  iteration. Each iteration can be thought of as the equivalent of a â€˜layerâ€™ in standard neural networks.

We will define a graph $G$ as a set of nodes, $V$ with a set of edges $E$ connecting them. Nodes can have individual features as part of the input: we will denote by $x_v$ the individual feature for node $v\in V$. For example, the â€˜node featuresâ€™ for a pixel in a color image would be the red, green and blue channel (RGB) values at that pixel.

Sometimes we will need to denote a graph property by a matrix $M$, where each row $M_v$ represents a property corresponding to a particular vertex $v$.



# Modern Graph Neural Networks

ChebNet was a breakthrough in learning localized filters over graphs, and it motivated many to think of graph convolutions from a different perspective.

We return back to the result of convolving $x$ by by the polynomial kernel $p_w(L)=L$ , focussing on a particular vertex $v$:
$$
\begin{align}
(Lx)_v
& = L_vx \\
& = \sum_{u\in G} L_{vu}x_u \\
& = \sum_{u\in G}(D_{vu}-A_{vu})x_u \\
& = D_vx_v-\sum_{u\in N(v)x_u}
\end{align}
$$
As we noted before, this is a 1-hop localized convolution. But more importantly, we can think of this convolution as arising of two steps:

- Aggregating over immediate neighbour features $x_u$
- Combining with the nodeâ€™s own feature $x_v$

**Key Idea:** What if we consider different kinds of â€˜aggregationâ€™ and â€˜combinationâ€™ steps, beyond what are possible using polynomial filters?

By ensuring that the aggregation is node-order equivariant, the overall convolution becomes node-order equivariant.

These convolutions can be thought of as â€˜message-passingâ€™ between adjacent nodes: after each step, every node receives some â€˜informationâ€™ from its neighbours.

By iteratively repeating the 1-hop localized convolutions $K$ times (i.e., repeatedly â€˜passing messagesâ€™), the receptive field of the convolution effectively includes all nodes upto $K$ hops away.

Message-passing forms the backbone of many GNN architectures today. We describe the most popular ones in depth below:

## Graph Convolutional Networks (GCN)

![](https://github.com/CorneliusDeng/Markdown-Photos/blob/main/Graph%20Neural%20Networks/Graph%20Convolutional%20Networks.png?raw=true)

## Graph Attention Networks (GAT)

![](https://github.com/CorneliusDeng/Markdown-Photos/blob/main/Graph%20Neural%20Networks/Graph%20Attention%20Networks.png?raw=true)

## Graph Sample and Aggregate (GraphSAGE)

![](https://github.com/CorneliusDeng/Markdown-Photos/blob/main/Graph%20Neural%20Networks/Graph%20Sample%20and%20Aggregate.png?raw=true)

## Graph Isomorphism Network (GIN)

![](https://github.com/CorneliusDeng/Markdown-Photos/blob/main/Graph%20Neural%20Networks/Graph%20Isomorphism%20Network.png?raw=true)

## Learning GNN Parameters

All of the embedding computations weâ€™ve described here, whether spectral or spatial, are completely differentiable. This allows GNNs to be trained in an end-to-end fashion, just like a standard neural network, once a suitable loss function $L$ is defined:

- **Node Classification**: By minimizing any of the standard losses for classification tasks, such as categorical cross-entropy when multiple classes are present:
  $$
  L(y_v,\widehat{y}_v)=\sum_cy_{vc}log\;\widehat{y}_{vc}
  $$
  where $\widehat{y}_{vc}$ is the predicted probability that node $v$ is in class $c$. GNNs adapt well to the semi-supervised setting, which is when only some nodes in the graph are labelled. In this setting, one way to define a loss $L_G$ over an input graph $G$ is:
  $$
  L_G=\frac{\sum_{v\in Lab(G)}L(y_v,\widehat{y}_v)}{|Lab(G)|}
  $$
  where, we only compute losses over labelled nodes $Lab(G)$.

- **Graph Classification**: By aggregating node representations, one can construct a vector representation of the entire graph. This graph representation can be used for any graph-level task, even beyond classification.

- **Link Prediction**: By sampling pairs of adjacent and non-adjacent nodes, and use these vector pairs as inputs to predict the presence/absence of an edge. For a concrete example, by minimizing the following â€˜logistic regressionâ€™-like loss:
  $$
  \begin{align}
  L(y_v,y_u,e_{vu}) & =-e_{vu}log(p_{vu})-(1-e_{vu})log(1-p_{vu}) \\
  p_{vu} & = \sigma(y_v^Ty_u)
  \end{align}
  $$
  where $\sigma$ is the sigmoid function, and $e_{vu} = 1$ iff there is an edge between nodes $v$ and $u$, being 0 otherwise.

- **Node Clustering**: By simply clustering the learned node representations.

The broad success of pre-training for natural language processing models such as ELMo and BERT has sparked interest in similar techniques for GNNs . The key idea in each of these papers is to train GNNs to predict local (eg. node degrees, clustering coefficient, masked node attributes) and/or global graph properties (eg. pairwise distances, masked global attributes).

Another self-supervised technique is to enforce that neighbouring nodes get similar embeddings, mimicking random-walk approaches such as node2vec and DeepWalk :
$$
L_G=\sum_v\sum_{u\in N_R(v)}log\frac{exp\;z_v^Tz_u}{exp\;z^T_{u'}z_u}
$$
where $N_R(v)$ is a multi-set of nodes visited when random walks are started from $v$. For large graphs, where computing the sum over all nodes may be computationally expensive, techniques such as Noise Contrastive Estimation are especially useful.



# Graph Convolutional Networks

## Extending Convolutions to Graphs

Convolutional Neural Networks have been seen to be quite powerful in extracting features from images. However, images themselves can be seen as graphs with a very regular grid-like structure, where the individual pixels are nodes, and the RGB channel values at each pixel as the node features.

A natural idea, then, is to consider generalizing convolutions to arbitrary graphs. However, ordinary convolutions are not node-order invariant, because they depend on the absolute positions of pixels. It is initially unclear as how to generalize convolutions over grids to convolutions over general graphs, where the neighbourhood structure differs from node to node.

Convolutions in CNNs are inherently localized. GNNs can perform localized convolutions mimicking CNNs.

CNNä¸­çš„å·ç§¯æœ¬è´¨ä¸Šå°±æ˜¯åˆ©ç”¨ä¸€ä¸ªå…±äº«å‚æ•°çš„è¿‡æ»¤å™¨(Kernel)ï¼Œ**é€šè¿‡è®¡ç®—ä¸­å¿ƒåƒç´ ç‚¹ä»¥åŠç›¸é‚»åƒç´ ç‚¹çš„åŠ æƒå’Œæ¥æ„æˆ feature map å®ç°ç©ºé—´ç‰¹å¾çš„æå–**ï¼Œå½“ç„¶åŠ æƒç³»æ•°å°±æ˜¯å·ç§¯æ ¸çš„æƒé‡ç³»æ•°ã€‚**ç¦»æ•£å·ç§¯æœ¬è´¨å°±æ˜¯ä¸€ç§åŠ æƒæ±‚å’Œ**

**å·ç§¯æ ¸çš„å‚æ•°é€šè¿‡ä¼˜åŒ–æ±‚å‡ºæ‰èƒ½å®ç°ç‰¹å¾æå–çš„ä½œç”¨ï¼ŒGCNçš„ç†è®ºå¾ˆå¤§ä¸€éƒ¨åˆ†å·¥ä½œå°±æ˜¯ä¸ºäº†å¼•å…¥å¯ä»¥ä¼˜åŒ–çš„å·ç§¯å‚æ•°**

GCNçš„æœ¬è´¨ç›®çš„å°±æ˜¯ç”¨æ¥æå–æ‹“æ‰‘å›¾çš„ç©ºé—´ç‰¹å¾ï¼Œé™¤äº† graph convolutionè¿™ä¸€ç§é€”å¾„å¤–ï¼Œåœ¨ vertex domain(spatial domain) å’Œ spectral domain å®ç°ç›®æ ‡æ˜¯ä¸¤ç§æœ€ä¸»æµçš„æ–¹å¼ã€‚

- **ç©ºé—´ç»´åº¦**

  Vertex domain(spatial domain) æ˜¯éå¸¸ç›´è§‚çš„ä¸€ç§æ–¹å¼ã€‚é¡¾åæ€ä¹‰ï¼šæå–æ‹“æ‰‘å›¾ä¸Šçš„ç©ºé—´ç‰¹å¾ï¼Œé‚£ä¹ˆå°±æŠŠæ¯ä¸ªé¡¶ç‚¹ç›¸é‚»çš„ neighbors æ‰¾å‡ºæ¥ï¼Œå…¶ä¸­è•´å«äº†ä¸¤ä¸ªå­é—®é¢˜ï¼š

  a. æŒ‰ç…§ä»€ä¹ˆæ¡ä»¶å»æ‰¾ä¸­å¿ƒvertexçš„neighborsï¼Œä¹Ÿå°±æ˜¯å¦‚ä½•ç¡®å®š receptive field

  b. ç¡®å®šreceptive fieldï¼ŒæŒ‰ç…§ä»€ä¹ˆæ–¹å¼å¤„ç†åŒ…å«ä¸åŒæ•°ç›®neighborsçš„ç‰¹å¾

  [Learning Convolutional Neural Networks for Graphs](http://proceedings.mlr.press/v48/niepert16.pdf) ç»™å‡ºçš„æ–¹æ³•ä¸»è¦ç¼ºç‚¹å¦‚ä¸‹ï¼š

  æ¯ä¸ªé¡¶ç‚¹æå–å‡ºæ¥çš„neighborsä¸åŒï¼Œä½¿å¾—è®¡ç®—å¤„ç†å¿…é¡»é’ˆå¯¹æ¯ä¸ªé¡¶ç‚¹ï¼›æå–ç‰¹å¾çš„æ•ˆæœå¯èƒ½æ²¡æœ‰å·ç§¯å¥½

- **å›¾è°±ç»´åº¦**

  **Spectral domain** å°±æ˜¯GCNçš„ç†è®ºåŸºç¡€äº†ã€‚è¿™ç§æ€è·¯å°±æ˜¯å¸Œæœ›å€ŸåŠ©å›¾è°±çš„ç†è®ºæ¥å®ç°æ‹“æ‰‘å›¾ä¸Šçš„å·ç§¯æ“ä½œã€‚ä»æ•´ä¸ªç ”ç©¶çš„æ—¶é—´è¿›ç¨‹æ¥çœ‹ï¼šé¦–å…ˆç ”ç©¶GSPï¼ˆgraph signal processingï¼‰çš„å­¦è€…å®šä¹‰äº†graphä¸Šçš„Fourier Transformationï¼Œè¿›è€Œå®šä¹‰äº†graphä¸Šçš„Convolutionï¼Œæœ€åä¸æ·±åº¦å­¦ä¹ ç»“åˆæå‡ºäº†Graph Convolutional Network

  ä»vertex domainåˆ†æé—®é¢˜ï¼Œéœ€è¦é€èŠ‚ç‚¹ï¼ˆnode-wiseï¼‰çš„å¤„ç†ï¼Œè€Œå›¾ç»“æ„æ˜¯éæ¬§å¼çš„è¿æ¥å…³ç³»ï¼Œè¿™åœ¨å¾ˆå¤šåœºæ™¯ä¸‹ä¼šæœ‰æ˜æ˜¾çš„å±€é™ï¼Œè€Œspectral domainæ˜¯å°†é—®é¢˜è½¬æ¢åœ¨â€œé¢‘åŸŸâ€é‡Œåˆ†æï¼Œä¸å†éœ€è¦node-wiseçš„å¤„ç†ï¼Œå¯¹äºå¾ˆå¤šåœºæ™¯èƒ½å¸¦æ¥æ„æƒ³ä¸åˆ°çš„ä¾¿åˆ©ï¼Œä¹Ÿæˆä¸ºäº†GSPçš„åŸºç¡€

## Graph Convolution

å¯¹äºå›¾ $G=(V,E)$ ï¼Œå…¶LaplaciançŸ©é˜µçš„å®šä¹‰ä¸º $L=D-A$ï¼Œå…¶ä¸­ $L$ æ˜¯Laplacian çŸ©é˜µï¼Œ$D$ æ˜¯é¡¶ç‚¹çš„åº¦çŸ©é˜µï¼ˆå¯¹è§’çŸ©é˜µï¼‰ï¼Œå¯¹è§’çº¿ä¸Šå…ƒç´ ä¾æ¬¡ä¸ºå„ä¸ªé¡¶ç‚¹çš„åº¦ï¼Œ$A$ æ˜¯å›¾çš„é‚»æ¥çŸ©é˜µã€‚

<img src="https://picx.zhimg.com/80/v2-5f9cf5fdeed19b63e1079ed2b87617b4_1440w.webp?source=1940ef5c" style="zoom:100%;" />

- ä¸ºä»€ä¹ˆGCNè¦ç”¨æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µï¼Ÿ
  - æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µæ˜¯å¯¹ç§°çŸ©é˜µï¼Œå¯ä»¥è¿›è¡Œç‰¹å¾åˆ†è§£ï¼ˆè°±åˆ†è§£ï¼‰ï¼Œè¿™å°±å’ŒGCNçš„spectral domainå¯¹åº”ä¸Šäº†
  - æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µåªåœ¨ä¸­å¿ƒé¡¶ç‚¹å’Œä¸€é˜¶ç›¸è¿çš„é¡¶ç‚¹ä¸Šï¼ˆ1-hop neighborï¼‰æœ‰é0å…ƒç´ ï¼Œå…¶ä½™ä¹‹å¤„å‡ä¸º0
  - é€šè¿‡æ‹‰æ™®æ‹‰æ–¯ç®—å­ä¸æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µè¿›è¡Œç±»æ¯”

**GCNçš„æ ¸å¿ƒåŸºäºæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„è°±åˆ†è§£**

çŸ©é˜µçš„è°±åˆ†è§£ï¼Œç‰¹å¾åˆ†è§£ï¼Œå¯¹è§’åŒ–éƒ½æ˜¯åŒä¸€ä¸ªæ¦‚å¿µã€‚ä¸æ˜¯æ‰€æœ‰çš„çŸ©é˜µéƒ½å¯ä»¥ç‰¹å¾åˆ†è§£ï¼Œå…¶å……è¦æ¡ä»¶ä¸ºné˜¶æ–¹é˜µå­˜åœ¨nä¸ªçº¿æ€§æ— å…³çš„ç‰¹å¾å‘é‡ã€‚æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µæ˜¯åŠæ­£å®šå¯¹ç§°çŸ©é˜µï¼Œæœ‰å¦‚ä¸‹ä¸‰ä¸ªæ€§è´¨ï¼š

- å®å¯¹ç§°çŸ©é˜µä¸€å®šnä¸ªçº¿æ€§æ— å…³çš„ç‰¹å¾å‘é‡
- åŠæ­£å®šçŸ©é˜µçš„ç‰¹å¾å€¼ä¸€å®šéè´Ÿ
- å®å¯¹é˜µçŸ©é˜µçš„ç‰¹å¾å‘é‡æ€»æ˜¯å¯ä»¥åŒ–æˆä¸¤ä¸¤ç›¸äº’æ­£äº¤çš„æ­£äº¤çŸ©é˜µ

ç”±ä¸Šå¯ä»¥çŸ¥é“æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µä¸€å®šå¯ä»¥è°±åˆ†è§£ï¼Œä¸”åˆ†è§£åæœ‰ç‰¹æ®Šçš„å½¢å¼ï¼Œå¯¹äºæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µå…¶è°±åˆ†è§£ä¸ºï¼š
$$
L= U
\left(\begin{matrix}\lambda_1 & \\&\ddots \\ &&\lambda_n \end{matrix}\right)
U^{-1}
$$
å…¶ä¸­ $U=(\vec{u_1},\vec{u_2},\cdots,\vec{u_n})$ï¼Œæ˜¯åˆ—å‘é‡ä¸ºå•ä½ç‰¹å¾å‘é‡çš„çŸ©é˜µï¼Œä¹Ÿå°±è¯´ $\vec{u_l}$ æ˜¯åˆ—å‘é‡ã€‚ç”±äº $U$ æ˜¯æ­£äº¤çŸ©é˜µï¼Œå³ $UU^{T}=E$ ï¼Œ$E$ æ˜¯å•ä½çŸ©é˜µ

æ‰€ä»¥ç‰¹å¾åˆ†è§£åˆå¯ä»¥å†™æˆï¼š
$$
L= U\left(\begin{matrix}\lambda_1 & \\&\ddots \\ &&\lambda_n \end{matrix}\right) U^{T}
$$
æŠŠä¼ ç»Ÿçš„å‚…é‡Œå¶å˜æ¢ä»¥åŠå·ç§¯è¿ç§»åˆ°Graphä¸Šæ¥ï¼Œæ ¸å¿ƒå·¥ä½œå…¶å®å°±æ˜¯æŠŠæ‹‰æ™®æ‹‰æ–¯ç®—å­çš„ç‰¹å¾å‡½æ•° $e^{-i\omega t}$ å˜ä¸ºGraphå¯¹åº”çš„æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„ç‰¹å¾å‘é‡ã€‚

### Graphä¸Šçš„å‚…é‡Œå¶å˜æ¢

ä¼ ç»Ÿçš„å‚…é‡Œå¶å˜æ¢å®šä¹‰ä¸ºï¼š$F(\omega)=\mathcal{F}[f(t)]=\int_{}^{}f(t)e^{-i\omega t} dt$ï¼Œå°±æ˜¯ ä¿¡å· $f(t)$ ä¸åŸºå‡½æ•° $e^{-i\omega t}$ çš„ç§¯åˆ†ï¼Œä»æ•°å­¦ä¸Šçœ‹ï¼Œ$e^{-i\omega t}$ æ˜¯æ‹‰æ™®æ‹‰æ–¯ç®—å­çš„ç‰¹å¾å‡½æ•°ï¼ˆæ»¡è¶³ç‰¹å¾æ–¹ç¨‹ï¼‰, $\omega$ å°±å’Œç‰¹å¾å€¼æœ‰å…³ã€‚

å¹¿ä¹‰çš„ç‰¹å¾æ–¹ç¨‹å®šä¹‰ä¸ºï¼š$A V=\lambda V$ï¼Œå…¶ä¸­ $A$ æ˜¯ä¸€ç§å˜æ¢ï¼Œ$V$ æ˜¯ç‰¹å¾å‘é‡æˆ–è€…ç‰¹å¾å‡½æ•°ï¼ˆæ— ç©·ç»´çš„å‘é‡ï¼‰ï¼Œ$\lambda$ æ˜¯ç‰¹å¾å€¼ã€‚

$e^{-i\omega t}$ æ»¡è¶³ï¼š$\Delta e^{-i\omega t}=\frac{\partial^{2}}{\partial t^{2}} e^{-i\omega t}=-\omega^{2} e^{-i\omega t}$ï¼Œå½“ç„¶ $e^{-i\omega t}$ å°±æ˜¯å˜æ¢ $\Delta$ çš„ç‰¹å¾å‡½æ•°ï¼Œ$\omega$ å’Œç‰¹å¾å€¼å¯†åˆ‡ç›¸å…³ã€‚

é‚£ä¹ˆï¼Œå¯ä»¥è”æƒ³äº†ï¼Œå¤„ç†Graphé—®é¢˜çš„æ—¶å€™ï¼Œç”¨åˆ°æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µï¼ˆæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µå°±æ˜¯ç¦»æ•£æ‹‰æ™®æ‹‰æ–¯ç®—å­ï¼‰ï¼Œè‡ªç„¶å°±å»æ‰¾æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„ç‰¹å¾å‘é‡äº†ã€‚

$L$ æ˜¯æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µï¼Œ$V$ æ˜¯å…¶ç‰¹å¾å‘é‡ï¼Œè‡ªç„¶æ»¡è¶³ä¸‹å¼ï¼š$LV=\lambda V$

ç¦»æ•£ç§¯åˆ†å°±æ˜¯ä¸€ç§å†…ç§¯å½¢å¼ï¼Œä»¿ä¸Šå®šä¹‰Graphä¸Šçš„å‚…é‡Œå¶å˜æ¢ï¼š$F(\lambda_l)=\hat{f}(\lambda_l)=\sum_{i=1}^{N}{f(i) u_l^*(i)}$ï¼Œ$f$ æ˜¯Graphä¸Šçš„ $N$ ç»´å‘é‡ï¼Œ$f(i)$ ä¸Graphçš„é¡¶ç‚¹ä¸€ä¸€å¯¹åº”ï¼Œ$u_l(i)$ è¡¨ç¤ºç¬¬ $l$ ä¸ªç‰¹å¾å‘é‡çš„ç¬¬ $i$ ä¸ªåˆ†é‡ã€‚é‚£ä¹ˆç‰¹å¾å€¼ï¼ˆé¢‘ç‡ï¼‰$\lambda_l$ ä¸‹çš„ï¼Œ$f$ çš„Graph å‚…é‡Œå¶å˜æ¢å°±æ˜¯ä¸ $\lambda_l$ å¯¹åº”çš„ç‰¹å¾å‘é‡ $u_l$ è¿›è¡Œå†…ç§¯è¿ç®—ã€‚

åˆ©ç”¨çŸ©é˜µä¹˜æ³•å°†Graphä¸Šçš„å‚…é‡Œå¶å˜æ¢æ¨å¹¿åˆ°çŸ©é˜µå½¢å¼ï¼š
$$
\left(\begin{matrix} \hat{f}(\lambda_1)\\ \hat{f}(\lambda_2) \\ \vdots \\\hat{f}(\lambda_N)\end{matrix}\right)=\left(\begin{matrix}\ u_1(1) &u_1(2)& \dots &u_1(N) \\u_2(1) &u_2(2)&\dots &u_2(N)\\ \vdots &\vdots &\ddots & \vdots\\ u_N(1) &u_N(2)& \dots &u_N(N)\end{matrix}\right)\left(\begin{matrix}f(1)\\ f(2) \\ \vdots \\f(N) \end{matrix}\right)
$$
å³ $f$ åœ¨Graphä¸Šå‚…é‡Œå¶å˜æ¢çš„çŸ©é˜µå½¢å¼ä¸ºï¼š$\hat{f}=U^Tf $

### Graphä¸Šçš„å‚…é‡Œå¶é€†å˜æ¢

ç±»ä¼¼åœ°ï¼Œä¼ ç»Ÿçš„å‚…é‡Œå¶é€†å˜æ¢æ˜¯å¯¹é¢‘ç‡ $\omega$ æ±‚ç§¯åˆ†ï¼š$\mathcal{F}^{-1}[F(\omega)]=\frac{1}{2\Pi}\int_{}^{}F(\omega)e^{i\omega t} d\omega$

è¿ç§»åˆ°Graphä¸Šå˜ä¸ºå¯¹ç‰¹å¾å€¼ $\lambda_l$ æ±‚å’Œï¼š$f(i)=\sum_{l=1}^{N}{\hat{f}(\lambda_l) u_l(i)}$

åˆ©ç”¨çŸ©é˜µä¹˜æ³•å°†Graphä¸Šçš„å‚…é‡Œå¶é€†å˜æ¢æ¨å¹¿åˆ°çŸ©é˜µå½¢å¼ï¼š
$$
\left(\begin{matrix}f(1)\\ f(2) \\ \vdots \\f(N) \end{matrix}\right)= \left(\begin{matrix}u_1(1) &u_2(1)& \dots &u_N(1) \\u_1(2) &u_2(2)& \dots &u_N(2)\\ \vdots &\vdots&\ddots & \vdots\\ u_1(N) &u_2(N)& \dots &u_N(N) \end{matrix}\right)\left(\begin{matrix} \hat{f}(\lambda_1)\\ \hat{f}(\lambda_2) \\ \vdots \\\hat{f}(\lambda_N)\end{matrix}\right)
$$
å³ $f$ åœ¨Graphä¸Šå‚…é‡Œå¶é€†å˜æ¢çš„çŸ©é˜µå½¢å¼ä¸ºï¼š$f=U\hat{f}$

### æ¨å¹¿å·ç§¯

åœ¨ä¸Šé¢çš„åŸºç¡€ä¸Šï¼Œåˆ©ç”¨å·ç§¯å®šç†ç±»æ¯”æ¥å°†å·ç§¯è¿ç®—ï¼Œæ¨å¹¿åˆ°Graphä¸Šã€‚

å·ç§¯å®šç†ï¼šå‡½æ•°å·ç§¯çš„å‚…é‡Œå¶å˜æ¢æ˜¯å‡½æ•°å‚…ç«‹å¶å˜æ¢çš„ä¹˜ç§¯ï¼Œå³å¯¹äºå‡½æ•° $f(t)$ ä¸ $h(t)$ ä¸¤è€…çš„å·ç§¯æ˜¯å…¶å‡½æ•°å‚…ç«‹å¶å˜æ¢ä¹˜ç§¯çš„é€†å˜æ¢ï¼š
$$
f*h=\mathcal{F}^{-1}\left[ \hat{f}(\omega)\hat{h}(\omega) \right]=\frac{1}{2\Pi}\int_{}^{}\hat{f}(\omega)\hat{h}(\omega)e^{i\omega t} d\omega
$$
ç±»æ¯”åˆ°Graphä¸Šå¹¶æŠŠå‚…é‡Œå¶å˜æ¢çš„å®šä¹‰å¸¦å…¥ï¼Œ$f$ ä¸å·ç§¯æ ¸ $h$ åœ¨Graphä¸Šçš„å·ç§¯å¯æŒ‰ä¸‹åˆ—æ­¥éª¤æ±‚å‡ºï¼š

$f$ çš„å‚…é‡Œå¶å˜æ¢ä¸º $\hat{f}=U^Tf$

å·ç§¯æ ¸ $h$ çš„å‚…é‡Œå¶å˜æ¢å†™æˆå¯¹è§’çŸ©é˜µçš„å½¢å¼å³ä¸ºï¼š $\left(\begin{matrix}\hat h(\lambda_1) &\\&\ddots \\ &&\hat h(\lambda_n) \end{matrix}\right)$

$\hat{h}(\lambda_l)=\sum_{i=1}^{N}{h(i) u_l^*(i)}$ æ˜¯æ ¹æ®éœ€è¦è®¾è®¡çš„å·ç§¯æ ¸ $h$ åœ¨Graphä¸Šçš„å‚…é‡Œå¶å˜æ¢

ä¸¤è€…çš„å‚…ç«‹å¶å˜æ¢ä¹˜ç§¯å³ä¸ºï¼š$\left(\begin{matrix}\hat h(\lambda_1) & \\&\ddots \\ &&\hat{h}(\lambda_n) \end{matrix}\right)U^Tf$

å†ä¹˜ä»¥ $U$ æ±‚ä¸¤è€…å‚…ç«‹å¶å˜æ¢ä¹˜ç§¯çš„é€†å˜æ¢ï¼Œåˆ™æ±‚å‡ºå·ç§¯ï¼š$(f*h)_G= U\left(\begin{matrix}\hat h(\lambda_1) & \\&\ddots \\ &&\hat h(\lambda_n)\end{matrix}\right) U^Tf$

æ³¨ï¼šå¾ˆå¤šè®ºæ–‡ä¸­çš„Graphå·ç§¯å…¬å¼ä¸ºï¼š$(f*h)_G=U((U^Th)\odot(U^Tf)) $

$\odot$ è¡¨ç¤ºHadamard productï¼ˆå“ˆè¾¾é©¬ç§¯ï¼‰ï¼Œå¯¹äºä¸¤ä¸ªç»´åº¦ç›¸åŒçš„å‘é‡ã€çŸ©é˜µã€å¼ é‡è¿›è¡Œå¯¹åº”ä½ç½®çš„é€å…ƒç´ ä¹˜ç§¯è¿ç®—ï¼Œå…¶å®ä¸¤å¼æ˜¯å®Œå…¨ç­‰ä»·çš„

### **ä¸ºä»€ä¹ˆæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„**ç‰¹å¾å‘é‡å¯ä»¥ä½œä¸ºå‚…é‡Œå¶å˜æ¢çš„åŸºï¼Ÿ

å‚…é‡Œå¶å˜æ¢ä¸€ä¸ªæœ¬è´¨ç†è§£å°±æ˜¯ï¼šæŠŠä»»æ„ä¸€ä¸ªå‡½æ•°è¡¨ç¤ºæˆäº†è‹¥å¹²ä¸ªæ­£äº¤å‡½æ•°ï¼ˆç”± $\sin\omega t,\cos\omega t$ æ„æˆï¼‰çš„çº¿æ€§ç»„åˆã€‚

<img src="https://picx.zhimg.com/80/v2-e9e00533154bfdad940e966e7eca5075_1440w.webp?source=1940ef5c" style="zoom:100%;" />

graphå‚…é‡Œå¶å˜æ¢ä¹ŸæŠŠgraphä¸Šå®šä¹‰çš„ä»»æ„å‘é‡ $f$ï¼Œè¡¨ç¤ºæˆäº†æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µç‰¹å¾å‘é‡çš„çº¿æ€§ç»„åˆï¼Œå³ï¼š$f=\hat{f}(\lambda_1)u_1+\hat{f}(\lambda_2)u_2+\cdots +\hat{f}(\lambda_n)u_n$

é‚£ä¹ˆï¼šä¸ºä»€ä¹ˆgraphä¸Šä»»æ„çš„å‘é‡ $f$ éƒ½å¯ä»¥è¡¨ç¤ºæˆè¿™æ ·çš„çº¿æ€§ç»„åˆï¼Ÿ

åŸå› åœ¨äº $(\vec{u_1},\vec{u_2},\cdots,\vec{u_n})$ æ˜¯graphä¸Š $n$ ç»´ç©ºé—´ä¸­çš„ $n$ ä¸ªçº¿æ€§æ— å…³çš„æ­£äº¤å‘é‡ï¼Œç”±çº¿æ€§ä»£æ•°çš„çŸ¥è¯†å¯ä»¥çŸ¥é“ï¼š$n$ ç»´ç©ºé—´ä¸­ $n$ ä¸ªçº¿æ€§æ— å…³çš„å‘é‡å¯ä»¥æ„æˆç©ºé—´çš„ä¸€ç»„åŸºï¼Œè€Œä¸”æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„ç‰¹å¾å‘é‡è¿˜æ˜¯ä¸€ç»„æ­£äº¤åŸºã€‚

æ­¤å¤–ï¼Œå¯¹äºä¼ ç»Ÿçš„å‚…é‡Œå¶å˜æ¢ï¼Œæ‹‰æ™®æ‹‰æ–¯ç®—å­çš„ç‰¹å¾å€¼ $\omega$ è¡¨ç¤ºè°æ³¢ $\sin\omega t,\cos\omega t$ çš„é¢‘ç‡ã€‚ä¸ä¹‹ç±»ä¼¼ï¼Œæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„ç‰¹å¾å€¼ $\lambda_i$ ä¹Ÿè¡¨ç¤ºå›¾æ‹‰æ™®æ‹‰æ–¯å˜æ¢çš„é¢‘ç‡ã€‚

## Graph Convolution Neural Network

Deep Learning ä¸­çš„ Convolution å°±æ˜¯è¦è®¾è®¡å«æœ‰ trainable å…±äº«å‚æ•°çš„ kernelï¼Œè€Œ Graph Convolution ä¸­çš„å·ç§¯å‚æ•°å°±æ˜¯ $diag(\hat{h}(\lambda_l))$

###  The first generation GCN

[Spectral Networks and Locally Connected Networks on Graphs](https://arxiv.org/abs/1312.6203) ä¸­ç®€å•ç²—æš´åœ°æŠŠ $diag(\hat h(\lambda_l) )$ å˜æˆäº†å·ç§¯æ ¸ $diag(\theta_l )$ ï¼Œä¹Ÿå°±æ˜¯ï¼š
$$
y_{output}=\sigma \left(U g_\theta(\Lambda) U^T x \right) \\
g_\theta(\Lambda)=\left(\begin{matrix}\theta_1 &\\&\ddots \\ &&\theta_n\end{matrix}\right)
$$
å®ƒå°±æ˜¯æ ‡å‡†çš„ç¬¬ä¸€ä»£GCNä¸­çš„ layeräº†ï¼Œå…¶ä¸­ $\sigma(\cdot)$ æ˜¯æ¿€æ´»å‡½æ•°ï¼Œ$\Theta=({\theta_1},{\theta_2},\cdots,{\theta_n})$ å°±è·Ÿä¸‰å±‚ç¥ç»ç½‘ç»œä¸­çš„weightä¸€æ ·æ˜¯ä»»æ„çš„å‚æ•°ï¼Œé€šè¿‡åˆå§‹åŒ–èµ‹å€¼ç„¶ååˆ©ç”¨è¯¯å·®åå‘ä¼ æ’­è¿›è¡Œè°ƒæ•´ï¼Œ$x$ å°±æ˜¯graphä¸Šå¯¹åº”äºæ¯ä¸ªé¡¶ç‚¹çš„feature vectorï¼ˆç”±ç‰¹æ•°æ®é›†æå–ç‰¹å¾æ„æˆçš„å‘é‡ï¼‰ã€‚

ï¼ˆä¸ºé¿å…æ··æ·†ï¼Œè®° $g_\theta(\Lambda)$ æ˜¯å·ç§¯æ ¸ï¼Œ$U g_\theta(\Lambda) U^T$ çš„è¿ç®—ç»“æœä¸ºå·ç§¯è¿ç®—çŸ©é˜µï¼‰

- ç¬¬ä¸€ä»£çš„å‚æ•°æ–¹æ³•å­˜åœ¨ç€ä¸€äº›å¼Šç«¯ï¼šä¸»è¦åœ¨äºï¼š
  - æ¯ä¸€æ¬¡å‰å‘ä¼ æ’­ï¼Œéƒ½è¦è®¡ç®— $U$, $diag(\theta_l )$ åŠ $U^T$ ä¸‰è€…çš„çŸ©é˜µä¹˜ç§¯ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤§è§„æ¨¡çš„graphï¼Œè®¡ç®—çš„ä»£ä»·è¾ƒé«˜ï¼Œä¹Ÿå°±æ˜¯è®ºæ–‡ä¸­
    $\mathcal{O}(n^3)$ çš„è®¡ç®—å¤æ‚åº¦
  - å·ç§¯æ ¸ä¸å…·æœ‰spatial localization
  - å·ç§¯æ ¸éœ€è¦ n ä¸ªå‚æ•°

###  The second generation GCN

[Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering](https://proceedings.neurips.cc/paper_files/paper/2016/hash/04df4d434d481c5bb723be1b6df1ee65-Abstract.html) æŠŠ $\hat h(\lambda_l)$ å·§å¦™åœ°è®¾è®¡æˆäº† $\sum_{j=0}^K \alpha_j \lambda^j_l$ï¼Œä¹Ÿå°±æ˜¯ï¼š
$$
y_{output}=\sigma \left(U g_\theta(\Lambda) U^T x \right) \\
g_\theta(\Lambda)=\left(\begin{matrix}\sum_{j=0}^K \alpha_j \lambda^j_1 &\\&\ddots \\&& \sum_{j=0}^K \alpha_j \lambda^j_n \end{matrix}\right)
$$
ä¸Šé¢çš„å…¬å¼ä»¿ä½›è¿˜ä»€ä¹ˆéƒ½çœ‹ä¸å‡ºæ¥ï¼Œä¸‹é¢åˆ©ç”¨çŸ©é˜µä¹˜æ³•è¿›è¡Œå˜æ¢ï¼š$\left(\begin{matrix}\sum_{j=0}^K \alpha_j \lambda^j_1 &\\&\ddots \\ && \sum_{j=0}^K\alpha_j \lambda^j_n \end{matrix}\right)=\sum_{j=0}^K \alpha_j \Lambda^j$

è¿›è€Œå¯ä»¥å¯¼å‡ºï¼š$U \sum_{j=0}^K \alpha_j \Lambda^j U^T =\sum_{j=0}^K \alpha_j U\Lambda^j U^T =\sum_{j=0}^K \alpha_j L^j$ 

ä¸Šå¼æˆç«‹æ˜¯å› ä¸º $L^2=U \Lambda U^TU \Lambda U^T=U \Lambda^2 U^T$ ä¸” $U^T U=E$

é‚£ä¹ˆï¼Œç­‰å¼å˜æ¢ä¸º
$$
y_{output}=\sigma \left( \sum_{j=0}^{K-1} \alpha_j L^j x \right)
$$
å…¶ä¸­ $({\alpha_0},{\alpha_1},\cdots,{\alpha_{K-1}})$ æ˜¯ä»»æ„çš„å‚æ•°ï¼Œé€šè¿‡åˆå§‹åŒ–èµ‹å€¼ç„¶ååˆ©ç”¨è¯¯å·®åå‘ä¼ æ’­è¿›è¡Œè°ƒæ•´ã€‚

- è¿™æ ·è®¾è®¡çš„å·ç§¯æ ¸å…¶ä¼˜ç‚¹åœ¨äº
  - å·ç§¯æ ¸åªæœ‰ $K$ ä¸ªå‚æ•°ï¼Œä¸€èˆ¬ $K$ è¿œå°äº $n$ï¼Œå‚æ•°çš„å¤æ‚åº¦è¢«å¤§å¤§é™ä½äº†
  - çŸ©é˜µå˜æ¢åï¼Œç¥å¥‡åœ°å‘ç°ä¸éœ€è¦åšç‰¹å¾åˆ†è§£äº†ï¼Œç›´æ¥ç”¨æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ $L$ è¿›è¡Œå˜æ¢ã€‚ç„¶è€Œç”±äºè¦è®¡ç®— $L^j$ï¼Œè®¡ç®—å¤æ‚åº¦è¿˜æ˜¯ $\mathcal{O}(n^3)$
  - å·ç§¯æ ¸å…·æœ‰å¾ˆå¥½çš„spatial localizationï¼Œç‰¹åˆ«åœ°ï¼Œ$K$ å°±æ˜¯å·ç§¯æ ¸çš„receptive fieldï¼Œä¹Ÿå°±æ˜¯è¯´æ¯æ¬¡å·ç§¯ä¼šå°†ä¸­å¿ƒé¡¶ç‚¹K-hop neighborä¸Šçš„featureè¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œæƒç³»æ•°å°±æ˜¯ $\alpha_k$

æ›´ç›´è§‚åœ°çœ‹ï¼Œ$K=1$ å°±æ˜¯å¯¹æ¯ä¸ªé¡¶ç‚¹ä¸Šä¸€é˜¶neighborçš„featureè¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<img src="https://pic1.zhimg.com/80/v2-5f756da1ce39f38d408bd771a15c8ad3_1440w.webp?source=1940ef5c" style="zoom:100%;" />

åŒç†ï¼Œ$K=2$ çš„æƒ…å½¢å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<img src="https://picx.zhimg.com/80/v2-a13b82907a364c3707a18bb8572b3a63_1440w.webp?source=1940ef5c" style="zoom:100%;" />

### Chebyshev polynomials are used as convolution kernels

åœ¨GCNé¢†åŸŸä¸­ï¼Œåˆ©ç”¨Chebyshevå¤šé¡¹å¼ä½œä¸ºå·ç§¯æ ¸æ˜¯éå¸¸é€šç”¨çš„å½¢å¼ï¼Œæ­¤å¤„æš‚ä¸å±•å¼€



# Graph Attention Networks

[Graph Attention Networks](https://openreview.net/forum?id=rJXMpikCZ) 

GCNæ˜¯å¤„ç†transductiveä»»åŠ¡çš„ä¸€æŠŠåˆ©å™¨ï¼ˆtransductiveä»»åŠ¡æ˜¯æŒ‡ï¼šè®­ç»ƒé˜¶æ®µä¸æµ‹è¯•é˜¶æ®µéƒ½åŸºäºåŒæ ·çš„å›¾ç»“æ„ï¼‰ï¼Œç„¶è€ŒGCNæœ‰**ä¸¤å¤§å±€é™æ€§**æ˜¯ç»å¸¸è¢«è¯Ÿç—…çš„ï¼š

1. **æ— æ³•å®Œæˆinductiveä»»åŠ¡ï¼Œå³å¤„ç†åŠ¨æ€å›¾é—®é¢˜ã€‚**

   inductiveä»»åŠ¡æ˜¯æŒ‡ï¼šè®­ç»ƒé˜¶æ®µä¸æµ‹è¯•é˜¶æ®µéœ€è¦å¤„ç†çš„graphä¸åŒã€‚é€šå¸¸æ˜¯è®­ç»ƒé˜¶æ®µåªæ˜¯åœ¨å­å›¾ï¼ˆsubgraphï¼‰ä¸Šè¿›è¡Œï¼Œæµ‹è¯•é˜¶æ®µéœ€è¦å¤„ç†æœªçŸ¥çš„é¡¶ç‚¹ã€‚ï¼ˆunseen nodeï¼‰

2. **å¤„ç†æœ‰å‘å›¾çš„ç“¶é¢ˆï¼Œä¸å®¹æ˜“å®ç°åˆ†é…ä¸åŒçš„å­¦ä¹ æƒé‡ç»™ä¸åŒçš„neighbor**

åœ¨[Graph Attention Networks](https://arxiv.org/abs/1710.10903)ä¸­æåˆ°ï¼Œ**GATæœ¬è´¨ä¸Šå¯ä»¥æœ‰ä¸¤ç§è¿ç®—æ–¹å¼**

1. **Global graph attention**

   é¡¾åæ€ä¹‰ï¼Œå°±æ˜¯æ¯ä¸€ä¸ªé¡¶ç‚¹ $i$ éƒ½å¯¹äºå›¾ä¸Šä»»æ„é¡¶ç‚¹éƒ½è¿›è¡Œattentionè¿ç®—

   ä¼˜ç‚¹ï¼šå®Œå…¨ä¸ä¾èµ–äºå›¾çš„ç»“æ„ï¼Œå¯¹äºinductiveä»»åŠ¡æ— å‹åŠ›

   ç¼ºç‚¹ï¼šï¼ˆ1ï¼‰ä¸¢æ‰äº†å›¾ç»“æ„çš„è¿™ä¸ªç‰¹å¾ï¼Œæ— å¼‚äºè‡ªåºŸæ­¦åŠŸï¼Œæ•ˆæœå¯èƒ½ä¼šå¾ˆå·®ï¼ˆ2ï¼‰è¿ç®—é¢ä¸´ç€é«˜æ˜‚çš„æˆæœ¬

2. **Mask graph attention**

   æ³¨æ„åŠ›æœºåˆ¶çš„è¿ç®—åªåœ¨é‚»å±…é¡¶ç‚¹ä¸Šè¿›è¡Œ

   è€Œåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œä½œè€…ä¹Ÿæ˜¯é‡‡å–è¿™æ ·çš„æ–¹å¼

GATçš„è®¡ç®—æ­¥éª¤ä¸»è¦æœ‰ä¸¤æ­¥ï¼š

- **è®¡ç®—æ³¨æ„åŠ›ç³»æ•°ï¼ˆattention coefficientï¼‰**

  å¯¹äºé¡¶ç‚¹ $i$ï¼Œé€ä¸ªè®¡ç®—å®ƒçš„é‚»å±…ä»¬ï¼ˆ $j\in N_i$ ï¼‰å’Œå®ƒè‡ªå·±ä¹‹é—´çš„ç›¸ä¼¼ç³»æ•°
  $$
  e_{ij}=a([Wh_i||Wh_j]),j\in N_i
  $$
  è§£é‡Šï¼šé¦–å…ˆä¸€ä¸ªå…±äº«å‚æ•° $W$ çš„çº¿æ€§æ˜ å°„å¯¹äºé¡¶ç‚¹çš„ç‰¹å¾è¿›è¡Œäº†å¢ç»´ï¼Œå½“ç„¶è¿™æ˜¯ä¸€ç§å¸¸è§çš„ç‰¹å¾å¢å¼ºï¼ˆfeature augmentï¼‰æ–¹æ³•ï¼›$[\cdot || \cdot]$ å¯¹äºé¡¶ç‚¹ $i,j$ çš„å˜æ¢åçš„ç‰¹å¾è¿›è¡Œäº†æ‹¼æ¥ï¼ˆconcatenateï¼‰ï¼›æœ€å $a(\cdot)$ æŠŠæ‹¼æ¥åçš„é«˜ç»´ç‰¹å¾æ˜ å°„åˆ°ä¸€ä¸ªå®æ•°ä¸Šï¼Œä½œè€…æ˜¯é€šè¿‡ single-layer feedforward neural networkå®ç°çš„

  æ˜¾ç„¶å­¦ä¹ é¡¶ç‚¹ $i,j$ ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå°±æ˜¯é€šè¿‡å¯å­¦ä¹ çš„å‚æ•° $W$ å’Œæ˜ å°„ $a(\cdot)$ å®Œæˆçš„

  æœ‰äº†ç›¸å…³ç³»æ•°ï¼Œå†é€šè¿‡ softmax å½’ä¸€åŒ–è®¡ç®—æ³¨æ„åŠ›ç³»æ•°
  $$
  \alpha_{ij}=\frac{exp(\text{Leaky ReLU}(e_{ij}))}{\sum_{k\in N_i}exp(\text{Leaky ReLU}(e_{ik}))}
  $$
  
- **åŠ æƒæ±‚å’Œï¼ˆaggregateï¼‰**

  æ ¹æ®è®¡ç®—å¥½çš„æ³¨æ„åŠ›ç³»æ•°ï¼ŒæŠŠç‰¹å¾åŠ æƒæ±‚å’Œï¼ˆaggregateï¼‰
  $$
  h'_i=\sigma(\sum_{j\in N_i}\alpha_{ij}Wh_j)
  $$
  
  å…¶ä¸­ï¼Œ$h'_i$ å°±æ˜¯ GAT è¾“å‡ºçš„å¯¹äºæ¯ä¸ªé¡¶ç‚¹ $i$ çš„æ–°ç‰¹å¾ï¼ˆèåˆäº†é‚»åŸŸä¿¡æ¯ï¼‰ï¼Œ $\sigma(\cdot)$ æ˜¯æ¿€æ´»å‡½æ•°ã€‚
  
  å†ä½¿ç”¨ **multi-head attention** è¿›ä¸€æ­¥å¢å¼º
  $$
  h'_i(K)= ||_{k=1}^K\; \sigma(\sum_{j\in N_i}\alpha_{ij}^kW^kh_j)
  $$
  å…¶ä¸­ï¼Œ$||$ è¡¨ç¤ºæ‹¼æ¥æ“ä½œï¼Œmulti-head attentionä¹Ÿå¯ä»¥ç†è§£æˆç”¨äº†ensembleçš„æ–¹æ³•

**å‡ ç‚¹æ·±å…¥ç†è§£**

- åœ¨æœ¬è´¨ä¸Šï¼ŒGCNä¸GATéƒ½æ˜¯å°†é‚»å±…é¡¶ç‚¹çš„ç‰¹å¾èšåˆåˆ°ä¸­å¿ƒé¡¶ç‚¹ä¸Šï¼ˆä¸€ç§aggregateè¿ç®—ï¼‰ï¼Œåˆ©ç”¨graphä¸Šçš„local stationaryå­¦ä¹ æ–°çš„é¡¶ç‚¹ç‰¹å¾è¡¨è¾¾ã€‚ä¸åŒçš„æ˜¯GCNåˆ©ç”¨äº†æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µï¼ŒGATåˆ©ç”¨attentionç³»æ•°ã€‚ä¸€å®šç¨‹åº¦ä¸Šè€Œè¨€ï¼ŒGATä¼šæ›´å¼ºï¼Œå› ä¸ºé¡¶ç‚¹ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§è¢«æ›´å¥½åœ°èå…¥åˆ°æ¨¡å‹ä¸­ã€‚

- GATé€‚ç”¨äºæœ‰å‘å›¾æœ€æ ¹æœ¬çš„åŸå› æ˜¯GATçš„è¿ç®—æ–¹å¼æ˜¯é€é¡¶ç‚¹çš„è¿ç®—ï¼ˆnode-wiseï¼‰ã€‚æ¯ä¸€æ¬¡è¿ç®—éƒ½éœ€è¦å¾ªç¯éå†å›¾ä¸Šçš„æ‰€æœ‰é¡¶ç‚¹æ¥å®Œæˆã€‚é€é¡¶ç‚¹è¿ç®—æ„å‘³ç€ï¼Œæ‘†è„±äº†æ‹‰æ™®åˆ©çŸ©é˜µçš„æŸç¼šï¼Œä½¿å¾—æœ‰å‘å›¾é—®é¢˜è¿åˆƒè€Œè§£ã€‚

- GATé€‚ç”¨äºinductiveä»»åŠ¡åŸå› æ˜¯ï¼ŒGATä¸­é‡è¦çš„å­¦ä¹ å‚æ•°æ˜¯ $W$ ä¸ $a(\cdot)$ï¼Œå› ä¸ºé€é¡¶ç‚¹è¿ç®—æ–¹å¼ï¼Œè¿™ä¸¤ä¸ªå‚æ•°ä»…ä¸é¡¶ç‚¹ç‰¹å¾ç›¸å…³ï¼Œä¸å›¾çš„ç»“æ„æ¯«æ— å…³ç³»ã€‚æ‰€ä»¥æµ‹è¯•ä»»åŠ¡ä¸­æ”¹å˜å›¾çš„ç»“æ„ï¼Œå¯¹äºGATå½±å“å¹¶ä¸å¤§ï¼Œåªéœ€è¦æ”¹å˜ $N_i$ï¼Œé‡æ–°è®¡ç®—å³å¯ã€‚

  ä¸æ­¤ç›¸åçš„æ˜¯ï¼ŒGCNæ˜¯ä¸€ç§å…¨å›¾çš„è®¡ç®—æ–¹å¼ï¼Œä¸€æ¬¡è®¡ç®—å°±æ›´æ–°å…¨å›¾çš„èŠ‚ç‚¹ç‰¹å¾ã€‚å­¦ä¹ çš„å‚æ•°å¾ˆå¤§ç¨‹åº¦ä¸å›¾ç»“æ„ç›¸å…³ï¼Œè¿™ä½¿å¾—GCNåœ¨inductiveä»»åŠ¡ä¸Šé‡åˆ°å›°å¢ƒã€‚



