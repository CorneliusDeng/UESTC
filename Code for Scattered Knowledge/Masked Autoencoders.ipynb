{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Patch Partition\n",
    "åˆ’åˆ†å®è´¨å°±æ˜¯ç»´åº¦çš„å˜æ¢\n",
    "'''\n",
    "\n",
    "num_patches = (h // self.patch_h) * (w // self.patch_w)\n",
    "# (b, c=3, h, w)->(b, n_patches, patch_size**2 * c)\n",
    "patches = x.view(\n",
    "    b, c,\n",
    "    h // self.patch_h, self.patch_h, \n",
    "    w // self.patch_w, self.patch_w\n",
    ").permute(0, 2, 4, 3, 5, 1).reshape(b, num_patches, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Masking\n",
    "æ ¹æ®é¢„è®¾çš„ mask æ¯”ä¾‹é‡‡ç”¨æœä»å‡åŒ€åˆ†å¸ƒçš„ç­–ç•¥éšæœºé‡‡æ ·ä¸€æ‰¹ patches å–‚ç»™ Encoderï¼Œå‰©ä¸‹çš„å°± mask æ‰\n",
    "'''\n",
    "\n",
    "# æ ¹æ® mask æ¯”ä¾‹è®¡ç®—éœ€è¦ mask æ‰çš„ patch æ•°é‡\n",
    "# num_patches = (h // self.patch_h) * (w // self.patch_w)\n",
    "num_masked = int(self.mask_ratio * num_patches)\n",
    "\n",
    "# Shuffle:ç”Ÿæˆå¯¹åº” patch çš„éšæœºç´¢å¼•\n",
    "# torch.rand() æœä»å‡åŒ€åˆ†å¸ƒ(normal distribution)\n",
    "# torch.rand() åªæ˜¯ç”Ÿæˆéšæœºæ•°ï¼Œargsort() æ˜¯ä¸ºäº†è·å¾—æˆç´¢å¼•\n",
    "# (b, n_patches)\n",
    "shuffle_indices = torch.rand(b, num_patches, device=device).argsort()\n",
    "# mask å’Œ unmasked patches å¯¹åº”çš„ç´¢å¼•\n",
    "mask_ind, unmask_ind = shuffle_indices[:, :num_masked], shuffle_indices[:, num_masked:]\n",
    "\n",
    "# å¯¹åº” batch ç»´åº¦çš„ç´¢å¼•ï¼š(b,1)\n",
    "batch_ind = torch.arange(b, device=device).unsqueeze(-1)\n",
    "# åˆ©ç”¨å…ˆå‰ç”Ÿæˆçš„ç´¢å¼•å¯¹ patches è¿›è¡Œé‡‡æ ·ï¼Œåˆ†ä¸º mask å’Œ unmasked ä¸¤ç»„\n",
    "mask_patches, unmask_patches = patches[batch_ind, mask_ind], patches[batch_ind, unmask_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Encoder å¯¹ un-masked çš„ patches è¿›è¡Œç¼–ç \n",
    "å…ˆå¯¹ un-masked patches è¿›è¡Œ emebdding è½¬æ¢æˆ tokensï¼Œå¹¶ä¸”åŠ ä¸Š position embeddingsï¼Œä»è€Œä¸ºå®ƒä»¬æ·»åŠ ä½ç½®ä¿¡æ¯ï¼Œç„¶åæ‰èƒ½æ˜¯çœŸæ­£çš„ç¼–ç è¿‡ç¨‹ã€‚è‡³äºç¼–ç è¿‡ç¨‹ï¼Œå®è´¨ä¸Šå°±æ˜¯æ‰”ç»™ Transformer ç© ğŸ˜„(query å’Œ key ç©ä¸€ç©ï¼Œç©å‡ºä¸ª attention åå†å’Œ value ä¸€èµ·ç©)\n",
    "'''\n",
    "\n",
    "# å°† patches é€šè¿‡ emebdding è½¬æ¢æˆ tokens\n",
    "unmask_tokens = self.encoder.patch_embed(unmask_patches)\n",
    "# ä¸º tokens åŠ å…¥ position embeddings \n",
    "# æ³¨æ„è¿™é‡Œç´¢å¼•åŠ 1æ˜¯å› ä¸ºç´¢å¼•0å¯¹åº” ViT çš„ cls_token\n",
    "unmask_tokens += self.encoder.pos_embed.repeat(b, 1, 1)[batch_ind, unmask_ind + 1]\n",
    "# çœŸæ­£çš„ç¼–ç è¿‡ç¨‹\n",
    "encoded_tokens = self.encoder.transformer(unmask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Decoder\n",
    "å°†ç¼–ç åçš„ tokens å’Œ æ·»åŠ äº†ä½ç½®ä¿¡æ¯åçš„ mask tokens æŒ‰åŸå…ˆå¯¹åº” patches çš„æ¬¡åºæ‹¼èµ·æ¥ï¼Œç„¶åå–‚ç»™ Decoder è§£ç \n",
    "è‹¥ç¼–ç åçš„ tokens ç»´åº¦è‹¥ä¸ Decoder è¦æ±‚çš„è¾“å…¥ç»´åº¦ä¸ä¸€è‡´ï¼Œéœ€è¦ä½¿ç”¨ linear projection è¿›è¡Œè½¬æ¢\n",
    "'''\n",
    "\n",
    "# å¯¹ç¼–ç åçš„ tokens ç»´åº¦è¿›è¡Œè½¬æ¢ï¼Œä»è€Œç¬¦åˆ Decoder è¦æ±‚çš„è¾“å…¥ç»´åº¦\n",
    "enc_to_dec_tokens = self.enc_to_dec(encoded_tokens)\n",
    "\n",
    "# ç”±äº mask token å®è´¨ä¸Šåªæœ‰1ä¸ªï¼Œå› æ­¤è¦å¯¹å…¶è¿›è¡Œæ‰©å±•ï¼Œä»è€Œå’Œ masked patches ä¸€ä¸€å¯¹åº”\n",
    "# (decoder_dim)->(b, n_masked, decoder_dim)\n",
    "mask_tokens = self.mask_embed[None, None, :].repeat(b, num_masked, 1)\n",
    "# ä¸º mask tokens åŠ å…¥ä½ç½®ä¿¡æ¯\n",
    "mask_tokens += self.decoder_pos_embed(mask_ind)\n",
    "\n",
    "# å°† mask tokens ä¸ ç¼–ç åçš„ tokens æ‹¼æ¥èµ·æ¥\n",
    "# (b, n_patches, decoder_dim)\n",
    "concat_tokens = torch.cat([mask_tokens, enc_to_dec_tokens], dim=1)\n",
    "# Un-shuffleï¼šæ¢å¤åŸå…ˆ patches çš„æ¬¡åº\n",
    "dec_input_tokens = torch.empty_like(concat_tokens, device=device)\n",
    "dec_input_tokens[batch_ind, shuffle_indices] = concat_tokens\n",
    "# å°†å…¨é‡ tokens å–‚ç»™ Decoder è§£ç \n",
    "decoded_tokens = self.decoder(dec_input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loss Computation\n",
    "å–å‡ºè§£ç åçš„ mask tokens é€å…¥å¤´éƒ¨è¿›è¡Œåƒç´ å€¼é¢„æµ‹ï¼Œç„¶åå°†é¢„æµ‹ç»“æœå’Œ masked patches æ¯”è¾ƒï¼Œè®¡ç®— MSE loss\n",
    "'''\n",
    "\n",
    "# å–å‡ºè§£ç åçš„ mask tokens\n",
    "dec_mask_tokens = decoded_tokens[batch_ind, mask_ind, :]\n",
    "# é¢„æµ‹ masked patches çš„åƒç´ å€¼\n",
    "# (b, n_masked, n_pixels_per_patch=patch_size**2 x c)\n",
    "pred_mask_pixel_values = self.head(dec_mask_tokens)\n",
    "# loss è®¡ç®—\n",
    "loss = F.mse_loss(pred_mask_pixel_values, mask_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Encoder ViT & Decoder Transformer\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def to_pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, net):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.net = net\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.net(self.norm(x), **kwargs)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, dim_per_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = dim_per_head ** -0.5\n",
    "\n",
    "        inner_dim = dim_per_head * num_heads\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "\n",
    "        project_out = not (num_heads == 1 and dim_per_head == dim)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, l, d = x.shape\n",
    "\n",
    "        '''i. QKV projection'''\n",
    "        # (b,l,dim_all_heads x 3)\n",
    "        qkv = self.to_qkv(x)\n",
    "        # (3,b,num_heads,l,dim_per_head)\n",
    "        qkv = qkv.view(b, l, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4).contiguous()\n",
    "        # 3 x (1,b,num_heads,l,dim_per_head)\n",
    "        q, k, v = qkv.chunk(3)\n",
    "        q, k, v = q.squeeze(0), k.squeeze(0), v.squeeze(0)\n",
    "\n",
    "        '''ii. Attention computation'''\n",
    "        attn = self.attend(\n",
    "            torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        )\n",
    "\n",
    "        '''iii. Put attention on Value & reshape'''\n",
    "        # (b,num_heads,l,dim_per_head)\n",
    "        z = torch.matmul(attn, v)\n",
    "        # (b,num_heads,l,dim_per_head)->(b,l,num_heads,dim_per_head)->(b,l,dim_all_heads)\n",
    "        z = z.transpose(1, 2).reshape(b, l, -1)\n",
    "        # assert z.size(-1) == q.size(-1) * self.num_heads\n",
    "\n",
    "        '''iv. Project out'''\n",
    "        # (b,l,dim_all_heads)->(b,l,dim)\n",
    "        out = self.out(z)\n",
    "        # assert out.size(-1) == d\n",
    "\n",
    "        return out\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, mlp_dim, depth=6, num_heads=8, dim_per_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, SelfAttention(dim, num_heads=num_heads, dim_per_head=dim_per_head, dropout=dropout)),\n",
    "                PreNorm(dim, FFN(dim, mlp_dim, dropout=dropout))\n",
    "            ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for norm_attn, norm_ffn in self.layers:\n",
    "            x = x + norm_attn(x)\n",
    "            x = x + norm_ffn(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self, image_size, patch_size, \n",
    "        num_classes=1000, dim=1024, depth=6, num_heads=8, mlp_dim=2048,\n",
    "        pool='cls', channels=3, dim_per_head=64, dropout=0., embed_dropout=0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        img_h, img_w = to_pair(image_size)\n",
    "        self.patch_h, self.patch_w = to_pair(patch_size)\n",
    "        assert not img_h % self.patch_h and not img_w % self.patch_w, \\\n",
    "            f'Image dimensions ({img_h},{img_w}) must be divisible by the patch size ({self.patch_h},{self.patch_w}).'\n",
    "        num_patches = (img_h // self.patch_h) * (img_w // self.patch_w)\n",
    "\n",
    "        assert pool in {'cls', 'mean'}, f'pool type must be either cls (cls token) or mean (mean pooling), got: {pool}'\n",
    "        \n",
    "        patch_dim = channels * self.patch_h * self.patch_w\n",
    "        self.patch_embed = nn.Linear(patch_dim, dim)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        # Add 1 for cls_token\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(p=embed_dropout)\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            dim, mlp_dim, depth=depth, num_heads=num_heads,\n",
    "            dim_per_head=dim_per_head, dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.pool = pool\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, img_h, img_w = x.shape\n",
    "        assert not img_h % self.patch_h and not img_w % self.patch_w, \\\n",
    "            f'Input image dimensions ({img_h},{img_w}) must be divisible by the patch size ({self.patch_h},{self.patch_w}).'\n",
    "        \n",
    "        '''i. Patch partition'''\n",
    "        num_patches = (img_h // self.patch_h) * (img_w // self.patch_w)\n",
    "        # (b,c,h,w)->(b,n_patches,patch_h*patch_w*c)\n",
    "        patches = x.view(\n",
    "            b, c, \n",
    "            img_h // self.patch_h, self.patch_h, \n",
    "            img_w // self.patch_w, self.patch_w\n",
    "        ).permute(0, 2, 4, 3, 5, 1).reshape(b, num_patches, -1)\n",
    "\n",
    "        '''ii. Patch embedding'''\n",
    "        # (b,n_patches,dim)\n",
    "        tokens = self.patch_embed(patches)\n",
    "        # (b,n_patches+1,dim)\n",
    "        tokens = torch.cat([self.cls_token.repeat(b, 1, 1), tokens], dim=1)\n",
    "        tokens += self.pos_embed[:, :(num_patches + 1)]\n",
    "        tokens = self.dropout(tokens)\n",
    "\n",
    "        '''iii. Transformer Encoding'''\n",
    "        enc_tokens = self.transformer(tokens)\n",
    "\n",
    "        '''iv. Pooling'''\n",
    "        # (b,dim)\n",
    "        pooled = enc_tokens[:, 0] if self.pool == 'cls' else enc_tokens.mean(dim=1)\n",
    "\n",
    "        '''v. Classification'''\n",
    "        # (b,n_classes)\n",
    "        logits = self.mlp_head(pooled)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reconstruction(Inference)\n",
    "'''\n",
    "\n",
    "@torch.no_grad\n",
    "def predict(self, x):\n",
    "    self.eval()\n",
    "\n",
    "    device = x.device\n",
    "    b, c, h, w = x.shape\n",
    "\n",
    "    '''i. Patch partition'''\n",
    "\n",
    "    num_patches = (h // self.patch_h) * (w // self.patch_w)\n",
    "    # (b, c=3, h, w)->(b, n_patches, patch_size**2*c)\n",
    "    patches = x.view(\n",
    "        b, c,\n",
    "        h // self.patch_h, self.patch_h, \n",
    "        w // self.patch_w, self.patch_w\n",
    "    ).permute(0, 2, 4, 3, 5, 1).reshape(b, num_patches, -1)\n",
    "\n",
    "    '''ii. Divide into masked & un-masked groups'''\n",
    "\n",
    "    num_masked = int(self.mask_ratio * num_patches)\n",
    "\n",
    "    # Shuffle\n",
    "    # (b, n_patches)\n",
    "    shuffle_indices = torch.rand(b, num_patches, device=device).argsort()\n",
    "    mask_ind, unmask_ind = shuffle_indices[:, :num_masked], shuffle_indices[:, num_masked:]\n",
    "\n",
    "    # (b, 1)\n",
    "    batch_ind = torch.arange(b, device=device).unsqueeze(-1)\n",
    "    mask_patches, unmask_patches = patches[batch_ind, mask_ind], patches[batch_ind, unmask_ind]\n",
    "\n",
    "    '''iii. Encode'''\n",
    "\n",
    "    unmask_tokens = self.encoder.patch_embed(unmask_patches)\n",
    "    # Add position embeddings\n",
    "    unmask_tokens += self.encoder.pos_embed.repeat(b, 1, 1)[batch_ind, unmask_ind + 1]\n",
    "    encoded_tokens = self.encoder.transformer(unmask_tokens)\n",
    "\n",
    "    '''iv. Decode'''\n",
    "    \n",
    "    enc_to_dec_tokens = self.enc_to_dec(encoded_tokens)\n",
    "\n",
    "    # (decoder_dim)->(b, n_masked, decoder_dim)\n",
    "    mask_tokens = self.mask_embed[None, None, :].repeat(b, num_masked, 1)\n",
    "    # Add position embeddings\n",
    "    mask_tokens += self.decoder_pos_embed(mask_ind)\n",
    "\n",
    "    # (b, n_patches, decoder_dim)\n",
    "    concat_tokens = torch.cat([mask_tokens, enc_to_dec_tokens], dim=1)\n",
    "    # dec_input_tokens = concat_tokens\n",
    "    dec_input_tokens = torch.empty_like(concat_tokens, device=device)\n",
    "    # Un-shuffle\n",
    "    dec_input_tokens[batch_ind, shuffle_indices] = concat_tokens\n",
    "    decoded_tokens = self.decoder(dec_input_tokens)\n",
    "\n",
    "    '''v. Mask pixel Prediction'''\n",
    "\n",
    "    dec_mask_tokens = decoded_tokens[batch_ind, mask_ind, :]\n",
    "    # (b, n_masked, n_pixels_per_patch=patch_size**2 x c)\n",
    "    pred_mask_pixel_values = self.head(dec_mask_tokens)\n",
    "\n",
    "    # æ¯”è¾ƒä¸‹é¢„æµ‹å€¼å’ŒçœŸå®å€¼\n",
    "    mse_per_patch = (pred_mask_pixel_values - mask_patches).abs().mean(dim=-1)\n",
    "    mse_all_patches = mse_per_patch.mean()\n",
    "\n",
    "    print(f'mse per (masked)patch: {mse_per_patch} mse all (masked)patches: {mse_all_patches} total {num_masked} masked patches')\n",
    "    print(f'all close: {torch.allclose(pred_mask_pixel_values, mask_patches, rtol=1e-1, atol=1e-1)}')\n",
    "        \n",
    "    '''vi. Reconstruction'''\n",
    "\n",
    "    recons_patches = patches.detach()\n",
    "    # Un-shuffle (b, n_patches, patch_size**2 * c)\n",
    "    recons_patches[batch_ind, mask_ind] = pred_mask_pixel_values\n",
    "    # æ¨¡å‹é‡å»ºçš„æ•ˆæœå›¾\n",
    "    # Reshape back to image \n",
    "    # (b, n_patches, patch_size**2 * c)->(b, c, h, w)\n",
    "    recons_img = recons_patches.view(\n",
    "        b, h // self.patch_h, w // self.patch_w, \n",
    "        self.patch_h, self.patch_w, c\n",
    "    ).permute(0, 5, 1, 3, 2, 4).reshape(b, c, h, w)\n",
    "\n",
    "    mask_patches = torch.randn_like(mask_patches, device=mask_patches.device)\n",
    "    # mask æ•ˆæœå›¾\n",
    "    patches[batch_ind, mask_ind] = mask_patches\n",
    "    patches_to_img = patches.view(\n",
    "        b, h // self.patch_h, w // self.patch_w, \n",
    "        self.patch_h, self.patch_w, c\n",
    "    ).permute(0, 5, 1, 3, 2, 4).reshape(b, c, h, w)\n",
    "\n",
    "    return recons_img, patches_to_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "simple pipline\n",
    "'''\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# è¯»å…¥å›¾åƒå¹¶ç¼©æ”¾åˆ°é€‚åˆæ¨¡å‹è¾“å…¥çš„å°ºå¯¸\n",
    "from PIL import Image\n",
    "\n",
    "img_raw = Image.open('test.jpg')\n",
    "h, w = img_raw.height, img_raw.width\n",
    "ratio = h / w\n",
    "print(f\"image hxw: {h} x {w} mode: {img_raw.mode}\")\n",
    "\n",
    "img_size, patch_size = (224, 224), (16, 16)\n",
    "img = img_raw.resize(img_size)\n",
    "rh, rw = img.height, img.width\n",
    "print(f'resized image hxw: {rh} x {rw} mode: {img.mode}')\n",
    "img.save('resized_test.jpg')\n",
    "\n",
    "# å°†å›¾åƒè½¬æ¢æˆå¼ é‡\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "img_ts = ToTensor()(img).unsqueeze(0).to(device)\n",
    "print(f\"input tensor shape: {img_ts.shape} dtype: {img_ts.dtype} device: {img_ts.device}\")\n",
    "\n",
    "# å®ä¾‹åŒ–æ¨¡å‹å¹¶åŠ è½½è®­ç»ƒå¥½çš„æƒé‡\n",
    "encoder = ViT(img_size, patch_size, dim=512, mlp_dim=1024, dim_per_head=64)\n",
    "decoder_dim = 512\n",
    "mae = MAE(encoder, decoder_dim, decoder_depth=6)\n",
    "weight = torch.load('mae.pth', map_location='cpu')\n",
    "mae.to(device)\n",
    "\n",
    "# æ¨ç†\n",
    "# æ¨¡å‹é‡å»ºçš„æ•ˆæœå›¾ï¼Œmask æ•ˆæœå›¾\n",
    "recons_img_ts, masked_img_ts = mae.predict(img_ts)\n",
    "recons_img_ts, masked_img_ts = recons_img_ts.cpu().squeeze(0), masked_img_ts.cpu().squeeze(0)\n",
    "\n",
    "# å°†ç»“æœä¿å­˜ä¸‹æ¥ä»¥ä¾¿å’ŒåŸå›¾æ¯”è¾ƒ\n",
    "recons_img = ToPILImage()(recons_img_ts)\n",
    "recons_img.save('recons_test.jpg')\n",
    "\n",
    "masked_img = ToPILImage()(masked_img_ts)\n",
    "masked_img.save('masked_test.jpg')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
