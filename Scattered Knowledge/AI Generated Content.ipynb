{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://mp.weixin.qq.com/s/gkTYUvICBpmGtqj9mL_w0A\n",
    "\n",
    "AIGCå³AI Generated Contentï¼Œæ˜¯æŒ‡åˆ©ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯æ¥ç”Ÿæˆå†…å®¹ï¼ŒAIGCä¹Ÿè¢«è®¤ä¸ºæ˜¯ç»§UGCã€PGCä¹‹åçš„æ–°å‹å†…å®¹ç”Ÿäº§æ–¹å¼ï¼ŒAIç»˜ç”»ã€AIå†™ä½œç­‰éƒ½å±äºAIGCçš„åˆ†æ”¯ã€‚\n",
    "\n",
    "æœ¬æ¬¡æ¯”èµ›çš„ä»»åŠ¡æ˜¯åˆ›å»ºä¸€ä¸ªæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥åè½¬ç”Ÿæˆç»™å®šå›¾åƒçš„æ‰©æ•£è¿‡ç¨‹ï¼ˆå³é€šè¿‡å›¾ç‰‡ç”ŸæˆåŸå§‹promptï¼‰ã€‚\n",
    "\n",
    "è¾“å…¥ï¼š16,000å¼ SDç”Ÿæˆçš„å›¾ç‰‡ï¼›\n",
    "\n",
    "æ ‡ç­¾ï¼špromptæ–‡æœ¬ç¼–ç ä¹‹åçš„å‘é‡ï¼›\n",
    "\n",
    "https://www.kaggle.com/competitions/stable-diffusion-image-to-prompts\n",
    "\n",
    "ä½¿ç”¨é¢„æµ‹å’Œå®é™…æç¤ºåµŒå…¥å‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦è¯„ä¼°ï¼Œå¯¹äºæµ‹è¯•é›†ä¸­çš„æ¯ä¸ªå›¾åƒï¼Œæ‚¨å¿…é¡»é¢„æµ‹ç”¨äºç”Ÿæˆå›¾åƒçš„promptè½¬æ¢ä¸º 384 é•¿åº¦çš„åµŒå…¥å‘é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ¸å¿ƒè¯„åˆ†ä»£ç å¦‚ä¸‹ï¼Œå¯¹æ–‡æœ¬é€šè¿‡all-MiniLM-L6-v2è¿›è¡Œç¼–ç ï¼Œç„¶åè½¬æ¢ç»´åº¦è¿›è¡Œæäº¤\n",
    "\n",
    "# https://www.kaggle.com/code/inversion/stable-diffusion-sample-submission/\n",
    "\n",
    "# è¯»å–promptæ–‡æœ¬\n",
    "sample_submission = pd.read_csv('sample_submission.csv', index_col='imgId_eId')\n",
    "\n",
    "# è¯»å–ç”¨äºç¼–ç promptçš„æ¨¡å‹\n",
    "st_model = SentenceTransformer('/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2')\n",
    "\n",
    "# ç¼–ç å¹¶è½¬æ¢ç»´åº¦\n",
    "prompt_embeddings = st_model.encode(prompts['prompt']).flatten()\n",
    "\n",
    "# å†™å…¥æäº¤ç»“æœ\n",
    "submission = pd.DataFrame(\n",
    "                index=imgId_eId,\n",
    "                data=prompt_embeddings,\n",
    "                columns=['val']).rename_axis('imgId_eId')\n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stable Diffusionä»‹ç»:\n",
    "\n",
    "Stable Diffusionï¼ˆç®€ç§°SDï¼‰è¯ç”Ÿäº2022å¹´8æœˆï¼Œä¸»è¦æ€è·¯æ¥è‡ªäºCVPR22 ä¸Šåˆä½œå‘è¡¨çš„æ½œæ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelï¼‰è®ºæ–‡ã€‚\n",
    "\n",
    "ç›¸æ¯”è¾ƒäºDALL-Eç­‰å¤§æ¨¡å‹ï¼ŒStable Diffusionå¯¹æ˜¾å­˜å‹åŠ›æ›´å°ã€‚Stable Diffusionä¸ä»…ç”Ÿæˆçš„å›¾åƒè´¨é‡éå¸¸é«˜ï¼Œè¿è¡Œé€Ÿåº¦å¿«ï¼Œå¹¶ä¸”æœ‰èµ„æºå’Œå†…å­˜çš„è¦æ±‚ä¹Ÿè¾ƒä½ã€‚\n",
    "\n",
    "Stable Diffusionæ ¹æ®æ–‡æœ¬æç¤ºä½œä¸ºè¾“å…¥æ¥ç”Ÿæˆçš„å›¾åƒï¼Œä¹Ÿå¯ä»¥ç”¨å®ƒå¯¹å›¾åƒæ ¹æ®æ–‡å­—æè¿°è¿›è¡Œä¿®æ”¹ã€‚\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SD Promptè§„åˆ™:\n",
    "\n",
    "Prompté¡ºåºï¼šè¶Šé å‰çš„Tagæƒé‡è¶Šå¤§ï¼šæ¯”å¦‚æ™¯è‰²Tagåœ¨å‰ï¼Œäººç‰©å°±ä¼šå°ï¼Œç›¸åçš„äººç‰©ä¼šå˜å¤§æˆ–åŠèº«ã€‚\n",
    "\n",
    "Promptä¸ªæ•°ï¼šç”Ÿæˆå›¾ç‰‡çš„å¤§å°ä¼šå½±å“Promptçš„æ•ˆæœï¼Œå›¾ç‰‡è¶Šå¤§éœ€è¦çš„Promptè¶Šå¤šï¼Œä¸ç„¶Promptä¼šç›¸äº’æ±¡æŸ“ã€‚\n",
    "\n",
    "Promptæƒé‡ï¼šåœ¨Stable Diffusion ä¸­ä½¿ç”¨()è‹±æ–‡æ‹¬å·å¯å¢åŠ æ‹¬å·ä¸­Tagåœ¨ç”»é¢ä¸­çš„æƒé‡ x1.1ï¼Œ[]å¯å‡å°Tagæƒé‡x0.91ã€‚\n",
    "\n",
    "Emoji Promptï¼šPromptæ”¯æŒä½¿ç”¨emojiï¼Œä¸”è¡¨ç°åŠ›è¾ƒå¥½ï¼Œå¯é€šè¿‡æ·»åŠ emojiè¾¾åˆ°è¡¨ç°æ•ˆæœï¼Œå¦‚ğŸ˜å½¢å®¹è¡¨æƒ…ã€‚\n",
    "\n",
    "æ›´å¤šSDç›¸å…³çš„Promptç‰¹å¾å·¥ç¨‹ï¼Œå¯ä»¥å‚è€ƒå¦‚ä¸‹èµ„æ–™ï¼š\n",
    "\n",
    "- https://github.com/Maks-s/sd-akashic\n",
    "- https://github.com/adieyal/sd-dynamic-prompts\n",
    "- https://invoke-ai.github.io/InvokeAI/features/PROMPTS/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all-MiniLM-L6-v2åœ¨å¯ç”¨è®­ç»ƒæ•°æ®ï¼ˆè¶…è¿‡ 10 äº¿ä¸ªå¥å­å¯¹ï¼‰ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå°†å¥å­å’Œæ®µè½æ˜ å°„åˆ° 384 ç»´å¯†é›†å‘é‡ç©ºé—´ï¼Œå¯ç”¨äºèšç±»æˆ–è¯­ä¹‰æœç´¢ç­‰ä»»åŠ¡ã€‚\n",
    "\n",
    "æ›´å¤šæ¨¡å‹ä»‹ç»ï¼Œå¯ä»¥å‚è€ƒå¦‚ä¸‹èµ„æ–™\n",
    "\n",
    "- https://www.sbert.net/docs/pretrained_models.html\n",
    "- https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "èµ›é¢˜æ˜¯AIGCé¢†åŸŸçš„å‰æ²¿èµ›é¢˜ï¼Œä½†å¹¶ä¸æ˜¯å•çº¯åˆ›é€ å›¾ç‰‡ï¼Œè€Œæ˜¯å¸Œæœ›æ„å»ºæ¨¡å‹å»ç”Ÿæˆåˆç†çš„promptï¼Œè¿›è€Œç”Ÿæˆåˆé€‚çš„å›¾ç‰‡ã€‚\n",
    "\n",
    "$$\n",
    "    å›¾ç‰‡---->prompt---->å›¾ç‰‡\n",
    "$$\n",
    "\n",
    "èµ›é¢˜ä½¿ç”¨çš„Stable Diffusion v2ï¼ˆSD v2ï¼‰æƒé‡æ˜¯å…¬å¼€çš„ï¼Œä¸”å¯ä»¥è½»æ¾åœ¨æœ¬åœ°æˆ–è€…è¿ç®—ç”Ÿæˆæ–°çš„å›¾ç‰‡ã€‚æ¨¡å‹æƒé‡å’Œåœ°å€å¦‚ä¸‹ï¼š\n",
    "\n",
    "- æ¨¡å‹æƒé‡ï¼šhttps://huggingface.co/stabilityai/stable-diffusion-2/blob/main/768-v-ema.ckpt\n",
    "- ä½¿ç”¨ä»£ç ï¼šhttps://github.com/Stability-AI/stablediffusion/blob/main/scripts/txt2img.py\n",
    "\n",
    "æœ¬æ¬¡èµ›é¢˜å¹¶ä¸æä¾›æ•°æ®ï¼Œè€Œæ˜¯æä¾›äº†ä¸€ä¸ªæ¨¡å‹æƒé‡ï¼Œéœ€è¦é€‰æ‰‹é€šè¿‡æ¨¡å‹æƒé‡æ¥å¾—åˆ°çš„ä¸€ä¸ªé€†å‘çš„æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€è·¯1ï¼šVit-GPT2\n",
    "# Visual Transformersï¼ˆVitï¼‰æ˜¯å¼ºå¤§çš„å›¾åƒåˆ†ç±»æ¨¡å‹ï¼Œ\n",
    "# è€Œ GPT2 æ˜¯ä¸€ç§å¯ç”¨äºç”Ÿæˆæ–‡æœ¬çš„è¯­è¨€æ¨¡å‹ã€‚Vit-GPT2æ€è·¯æ˜¯å°†å›¾ç‰‡Vitç‰¹å¾è§£ç ä¸ºæ–‡æœ¬ï¼Œç”¨æ¥æè¿°å›¾ç‰‡å†…å®¹\n",
    "# Baselineåœ°å€ï¼šhttps://huggingface.co/nlpconnect/vit-gpt2-image-captioning\n",
    "\n",
    "# åŠ è½½Vit-GPTæ¨¡å‹\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_dir)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# å¯¹å›¾ç‰‡ç”Ÿæˆæ–‡æœ¬ï¼Œç„¶åå¯¹æ–‡æœ¬è¿›è¡Œç¼–ç \n",
    "def predict_step(image_paths):\n",
    "  images = []\n",
    "  for image_path in image_paths:\n",
    "    i_image = Image.open(image_path)\n",
    "    if i_image.mode != \"RGB\":\n",
    "      i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "    images.append(i_image)\n",
    "  \n",
    "  # Vit-GPTï¼ŒVitéƒ¨åˆ†\n",
    "  pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "  pixel_values = pixel_values.to(device)\n",
    "  # Vit-GPTï¼ŒGPTéƒ¨åˆ†\n",
    "  output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "  # æ–‡æœ¬ç¼–ç éƒ¨åˆ†\n",
    "  preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "  preds = [pred.strip() for pred in preds]\n",
    "  return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€è·¯2ï¼šOFAæ¨¡å‹\n",
    "# OFAå°†å¤šæ¨¡æ€åŠå•æ¨¡æ€çš„ç†è§£å’Œç”Ÿæˆä»»åŠ¡ç»Ÿä¸€åˆ°ä¸€ä¸ªç®€å•çš„Seq2Seqç”Ÿæˆå¼æ¡†æ¶ä¸­ï¼ŒOFAæ‰§è¡Œé¢„è®­ç»ƒå¹¶ä½¿ç”¨ä»»åŠ¡æŒ‡ä»¤è¿›è¡Œå¾®è°ƒã€‚\n",
    "# OFAæ¨¡å‹èµ„æ–™å¦‚ä¸‹ï¼š\n",
    "# - å¼€æºåœ°å€ï¼šhttps://github.com/OFA-Sys/OFA\n",
    "# - æ¨¡å‹æƒé‡ï¼šhttps://huggingface.co/OFA-Sys\n",
    "# Baselineåœ°å€ï¼šhttps://www.kaggle.com/code/mayukh18/ofa-transformer-lb-0-42644\n",
    "\n",
    "# å›¾ç‰‡æ•°æ®å˜æ¢\n",
    "mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
    "resolution = 480\n",
    "patch_resize_transform = transforms.Compose([\n",
    "        lambda image: image.convert(\"RGB\"),\n",
    "        transforms.Resize((resolution, resolution), interpolation=Image.BICUBIC),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "# åŠ è½½OFAæ¨¡å‹ï¼Œè¾“å…¥ä»»åŠ¡æŒ‡ä»¤\n",
    "tokenizer = OFATokenizer.from_pretrained(CKPT_DIR)\n",
    "model = OFAModel.from_pretrained(CKPT_DIR, use_cache=False).cuda()\n",
    "txt = \" what does the image describe?\"\n",
    "inputs = tokenizer([txt], return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€è·¯3ï¼šCLIP + Promptæ¨¡æ¿\n",
    "# CLIPä»äº’è”ç½‘æ”¶é›†çš„4äº¿(å›¾åƒã€æ–‡æœ¬)å¯¹çš„æ•°æ®é›†ï¼Œåœ¨é¢„è®­ç»ƒä¹‹åï¼Œç”¨è‡ªç„¶è¯­è¨€æè¿°æ‰€å­¦çš„è§†è§‰æ¦‚å¿µï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨zero-shotçŠ¶æ€ä¸‹è½¬ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡ã€‚\n",
    "# Baselineåœ°å€ï¼šhttps://www.kaggle.com/code/leonidkulyk/lb-0-45836-blip-clip-clip-interrogator\n",
    "\n",
    "def interrogate(image: Image) -> str:\n",
    "    # CLIPé¢„æµ‹å¾—åˆ°æ–‡æœ¬\n",
    "    caption = ci.generate_caption(image)\n",
    "    \n",
    "    # è®¡ç®—å›¾ç‰‡ç‰¹å¾\n",
    "    image_features = ci.image_to_features(image)\n",
    "    \n",
    "    # è®¡ç®—åšç‰©é¦†prompt\n",
    "    medium = [ci.mediums.labels[i] for i in cos(image_features, mediums_features_array).topk(1).indices][0]\n",
    "    # è®¡ç®—å›¾ç‰‡é£æ ¼prompt\n",
    "    movement = [ci.movements.labels[i] for i in cos(image_features, movements_features_array).topk(1).indices][0]\n",
    "    # è®¡ç®—å£å‘³/é€šç”¨prompt\n",
    "    flaves = \", \".join([ci.flavors.labels[i] for i in cos(image_features, flavors_features_array).topk(3).indices])\n",
    "    \n",
    "    # promptæ¨¡æ¿\n",
    "    if caption.startswith(medium):\n",
    "        prompt = f\"{caption}, {movement}, {flaves}\"\n",
    "    else:\n",
    "        prompt = f\"{caption}, {medium}, {movement}, {flaves}\"\n",
    "\n",
    "    return clip_interrogator._truncate_to_fit(prompt, ci.tokenize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "welldone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "164ec7185b9f2c6d4334e977d6a0649173055921bb42a75b111790aaa12a8826"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
